# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Rbm5unyCdPRhpomhHCot4PYITbiwnB9

## **Import Libraries**
"""

!pip install wikipedia

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Activation, Input, Dot, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.datasets import imdb
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.manifold import TSNE
import wikipedia
import matplotlib.pyplot as plt
import pandas as pd
import copy
from math import *
# %matplotlib inline

"""## **Pre-Word2Vec Methods**"""

text = """
Pranu: Done jay mama I started my app installations with whatâ€™s app  Jay Mama: Hi Pranu  Pranu: Hi jay mama  Pranu: Will update my profile pic soon  Jay Mama: How to add u to Kala group  Jay Mama: Hey nice DP  Pranu: Thank you jay mama  Jay Mama: Down load Pinterest and Myntra app  Jay Mama: Start shopping  Pranu: Ok  Jay Mama: If want to buy any games buy it donâ€™t hesitate  Pranu: Thank u ,thank u ,thank u lovely jay mama  Pranu: I will use it when required  Pranu: Jay mama I got an alert that some one is trying to sign into my iCloud account.Is it something to worry??  Jay Mama: Send me that mail  Pranu: Not the mail it was a flash messenger  Pranu: *message  Jay Mama: Oh ok did u get any one time password  Pranu: No I got something like this â€œDo you want to allow access to so and so personâ€  Pranu: And I opted no donâ€™t allow  Jay Mama: Ok  Jay Mama: Look into this   Pranu: Thank u jay mama I will definitely look into this.  Jay Mama: Very useful stuff for you Pranu. Kindly read it when ur free   Pranu: This is the same message I got and when checked the YouTube I found that the message is false so can I just relax now? Or is there anything still to worry about??  Jay Mama: Donâ€™t install anything  Jay Mama: Call me  Pranu: Jay mama could u send me the account details of Netflix. I am not able to stream it on my tv due to poor internet connectivity with my router  Pranu: So I need account details to install the app and login into my shared Netflix account .  Jay Mama: Netflix account details :  Pranu: Thanks jay mama  Pranu: Pops up a message â€˜incorrect passwordâ€™  Jay Mama: Try again properly Pranu.  Pranu: Copy pasted it jay mama  Pranu: Same result  Jay Mama: Ok wait  Jay Mama: Changed Netflix password - jharicv@me.com, Password- JAYAcv5670!  Pranu: Worked!!! Thanks jay mama....  Pranu: Nanna cell ki phone chae mama  Jay Mama: Ok  Jay Mama: Give ur gmail is  Pranu: Thanks jay mama. I have seen it. Itâ€™s helpful  Jay Mama: Pranu sleep ra enough ra  Jay Mama: Thanks a lot nana. Will call u tomorrow  Pranu: No prob jay mama itâ€™s holidays  Jay Mama: Donâ€™t post anymore pics. People will be tried  Jay Mama: Just for fun nana  Pranu: Chill!!!! Jay mama  Pranu: Need not specify jay mama I aspire this humour attitude of yours  Jay Mama: Pranu do u have bank account  Pranu: Nooope  Pranu: Why?!  Jay Mama: After lockdown open. Open one bank pranu.  Jay Mama: I shall tell later  Pranu: Ok Jay mama  Pranu: Done ðŸ‘  Pranu: This above one is an intro editing done by me today for our short film  Pranu: This one has more editing elements in it and the banner that appears at the first is self created  Pranu: Jay mama could you call me now?  Pranu: Jay mama we though of giving vehicles, other than those that belong to Hyundai, a try as we wanted our car to be something new into the society like Honda City  or Kia Seltos or MG  Jay Mama: Ok nice  Jay Mama: Whoâ€™s is driving  Jay Mama: The car  Jay Mama: You or dad  Pranu: Well both  Pranu: Just like the i20  Pranu: Thoughts about my car havenâ€™t been pondered over yet  Pranu: Well even Venue is new into the society but yet weâ€™ll didnâ€™t like the model for unknown reasons  Jay Mama: Ya check on those website and videos in our budget  Pranu: Hmm.. currently on it in fact weâ€™re planning to pay a visit to the Honda City and Kia showrooms  Pranu: To have a clear set look at the cars  Jay Mama: Ok ðŸ‘ðŸ»  Pranu: Ok jay mama I think itâ€™s real mid night for u good night then bye  Jay Mama: Yes nana to tomorrow  Jay Mama: I have flights  Jay Mama: To Virginia  Pranu: Ohh  Pranu: Jay mama, yesterday after u said abt those anonymous logins into Babayeâ€™s account, I just intimated all my friends in a soft tone to double check that they are entering Netflix only through my profile, but they replied they never did enter any other profile and also i personally keep checks on them and their viewing history regularly. For sure these are not the guys and I guess it could have been Pranuâ€™s  friends. Just guess  Jay Mama: Ok no problem  Jay Mama: Itâ€™s all Part of youth life  Pranu: I know but just making it clear that itâ€™s not my friends and by the way, besides cautioning my friends about login I myself ironically logged into Babayeâ€™s account by mistake this morning! I know itâ€™s coming youth life as u said but this time itâ€™s not we  That was a punch line ðŸ˜ðŸ˜  Pranu: Okay GN jay mama, might have been really tiered  Pranu: jay mama, Found this on youtube  Pranu: though in this video it shows how to connect PC to IMac, it also works with a windows laptop  Pranu: Perks: No need of any ports or cables everything\'s done wirelessly  Pranu: Loop hole: I need to uninstall windows seven residing on my IMac and install windows 10 for all of this to happen I need to get my mac keyboard fixed in the first place  Jay Mama: Check the cable option pranu  Pranu: Iâ€™ll but why cable when it can be done without cable?  Pranu: We need a HDMI to Thunderbolt connector  Jay Mama: Pranu login to amazon venkatajay567@gmail.com, JAYAcv5670!  Jay Mama: To watch man who knew infinity  Pranu: Oh great!! thanks a jay mama  Jay Mama: Watch before 48 hours from now nana  Pranu: Sure jay mama!! Morning I have classes to take so Iâ€™ll watch it tonight  Jay Mama: .com  Pranu: Ok  Pranu: Otp pls  Jay Mama: 913035 is your Amazon OTP. Do not share it with anyone.  Pranu: Done jay mama  Jay Mama: Enjoy ðŸ˜Š  Pranu: Just now finished watching the movie. Thatâ€™s been a great movie to watch. My respect for him grew at least by a bit more after watching this film. All thanks to you Jay mama!!! This is one is a cracker of a Biopic for sure  Jay Mama: True ðŸ‘ðŸ»  Jay Mama: Pranu what is the app to create to video with music apart from iMovies  Pranu: Use Filmora jay mama  Pranu: Licensed e-mail: c2941647@drdrb.com Registration code: 10403029CF3644154841651AF141E800 Licensed e-mail: c2941690@drdrb.com Registration code: 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Registration code: 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2942269@drdrb.com Registration code: 00289623F7B3B81E14AEB526144B6D08 Licensed e-mail: bidjan@ziggo.nl  Key : CE8B0909EEC77B27DFEA94190F3A0223 Licensed e-mail: c2941690@drdrb.com Key: 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Key: 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2943267@drdrb.com Key: D772BE0279AFE60AF0E1D2109CA89A19 Licensed e-mail: c2943297@drdrb.com Key: FB9694298253B51545E70D22B3033808 Licensed e-mail: c2941647@drdrb.com Key: 10403029CF3644154841651AF141E800 Licensed e-mail: c2941690@drdrb.com  Key : 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Key : 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2942269@drdrb.com  Key : 00289623F7B3B81E14AEB526144B6D08 Licensed e-mail: c2943267@drdrb.com Key : D772BE0279AFE60AF0E1D2109CA89A19 Licensed e-mail: c2943297@drdrb.com Key : FB9694298253B51545E70D22B303380  Pranu: And jay mama use the above lisence mail IDs  Pranu: To remove water mark  Pranu: From the videos created using filmora with out having to subscribe for the paid version  Jay Mama: Nice DP pranu  Pranu: Thank u jay mama. so the DP is nice to be noticed....  Pranu: Hmm  Pranu: Ignore all those deleted messages Jay mama, they were meant to be sent to my friends just some pics of my code work  Jay Mama: Ok thanks pranu  Pranu: Raw!!! Yet funny. In fact Jay mama in our batch too there was a guy, he attended the college for the first 2-3 months of first year and disappeared suddenly and finally just before 2 months to IIT exams in second year he appeared. He was the centre of attraction then when sirs asked him what happened to him all these 1:5 years, he said like he was preparing for CA exams and now he wanted to prepare for JEE so he came  Pranu: Almost similar to answers given in the video but no cuss words  Pranu: Finally, lecturers took him in as he has a point â€œI paid fee now whatâ€™s ur problemâ€  Pranu: Though it doesnâ€™t work always, yet itâ€™s still a fun always to fool others. Isnâ€™t it??!!  Pranu: Jay mama where do u get your WhatsApp stickers from?  Jay Mama: Form whom ?  Jay Mama: From whom ?  Pranu: Like which app?  Pranu: There are very limited out there for iPhones right?  Jay Mama: Meme ?  Pranu: Yeah  Pranu: Like WhatsApp stickers  Jay Mama: Tenor  Pranu: Yeah this one  Pranu: Great thanks jay mama  Pranu: Latest news right Jay mama? This seems to be lot cooler than Xbox Series X. What do u say?  Pranu: Hmm everythingâ€™s great but then what abt Optical drive thingy Jay mama? We donâ€™t  have it in Xbox S right?  Pranu: Like what are the alternatives to the games if not CDS?  Pranu: Jay mama did u receive any mail regarding verification code from Tatasky  Pranu: If so could u send the code  Pranu: Asap my friend is accessing our Tatasky to watch IPL  Jay Mama: To which email Iâ€™d  Pranu: The one starting with J  Pranu: Any such?  Jay Mama: No pranu I didnâ€™t receive  Jay Mama: Why my email is used ?  Pranu: Ok jay mama donâ€™t know  Pranu: Like hold on  Pranu: This is what it showed  Pranu: The mail there started with j  Pranu: And Nanna is a out with low internet thatâ€™s why heâ€™s not receiving the OTPs  Jay Mama: Ok wait let me look deeply in Gmail  Pranu: Hmm only if u r free Jay mama  Jay Mama: Cancel and request again  Pranu: Ok on it  Pranu: Jay mama relax seems like my friend accessed wrong website he went to the accounts website  Pranu: Might get solved without ur intervention  Jay Mama: Oh no ðŸ¤¦\u200dâ™‚ï¸  Jay Mama: Ok let me know if any nice DP  Pranu: Thank u Jay mama  Pranu: Hmm Iâ€™ll make note of this Jay mama  Jay Mama: I think it would be expired  Pranu: Why? So should o reset the details again  Pranu: ?  Pranu: *I  Jay Mama: Request again  Jay Mama: I shall share the password  Pranu: Ok  Pranu: Jay mama u should have got it  Jay Mama: Password- ybg6T5  Pranu: Done jay mama  Pranu: Thanks  Pranu: I was waiting for the reset password to come to Nannaâ€™s phone but it didnâ€™t show up  Jay Mama: Cool  Pranu: And so were those multiple requests  Pranu: ðŸ˜‚ðŸ¤£ðŸ˜‡  Pranu: *last emoji typo  Pranu: ðŸ˜‚  Pranu: Super funny jay mama perfect lip sink  Pranu: The "Le Petit Chef " restaurant in France, came up with an original way to entertain guests while waiting for their order by using an overhead 3D projector on the ceiling.\xa0 The animation is on the table and your plate. There is a small chef who appears on your plate, and thatâ€™s only the beginning.  Pranu: Jay mama we feel the chair is more of a issue here than the table. So also send ur recommendations on chairs  Pranu: The chair u have now would do  Jay Mama: That chair is not available in ikea India Pranu  Pranu: Can we find the same model in any other stores  Pranu: Or any model similar to that  Pranu: I too will search now once  Pranu: Could u send me ur chair model so that I can search  Jay Mama: Really Epic ðŸ˜ƒ  Pranu: ðŸ˜‚ðŸ¤£  Pranu: Valid point to argue with  Pranu: ðŸ˜‚  Pranu: Jay mama Iâ€™ll send u the credentials In abt 20 mins or so having dinner  Jay Mama: Ok  Pranu: Mail ID : prasad5670@gmail.com Password : Hometheatre6  Pranu: Jay mama  Pranu: My keyboard thing we went through all the stores Apple non Apple but no use it needs to be replaced  Pranu: I have searched for the prices and these are the estimates : Non numerical keyboard  8900 Numerical Keyboard   Non Numerical Keyboard  Numerical keyboard  I prefer Numerical cause itâ€™s the latest one and also chargeable no head ache of batteries being corroded or stuck again  Pranu: So which way is it? Shall I buy it here or will u buy there?  Pranu: Jay mama placed order for a 300 rs USB keyboard  Pranu: And also checked out and found that we can connect through USB keyboards at login  Pranu: So this should work  Jay Mama: Super  Pranu: And also Guest user account is locked from inside so thatâ€™s out of thought now  Pranu: Jay mama got the USB keyboard and itâ€™s working  Pranu: So Jay mama suggest a nice Bluetooth keyboard  Jay Mama: Will look into  Jay Mama: Super  Pranu: ðŸ‘  Pranu: Jay mama  Pranu: Have a look at this  Pranu: I pretty much like it  Jay Mama: Ya good will that keyboard work for  coding ?  Pranu: Yep  Jay Mama: How much  Pranu: 52$  Pranu: 3744 rs  Pranu: Searching for Indian price  Pranu: And stock  Pranu: Ya like price tag mentioned in dollars but deliver to India option available  Jay Mama: Ok let me check  Pranu: Jay mama Indian price 11000  Pranu: I think I should find some other model in Logitech itself I pretty much liked their button design  Pranu: Check out "Oke Oka Jeevitham" from Mr. Nookayya on JioSaavn!  Pranu: Jay mama we are planning to replace our old TV with a new one!! Do u know of any new models? If free call  Jay Mama: Oh nice  Jay Mama: will call you nana  Pranu: Ya ok jay mama no hurry  Pranu: Jay mama my old profile in Netflix is gone however I have an option to add profile!! Iâ€™m creating a new one under your account  Pranu: Jay mama waiting for your feed  Pranu: Jay mama how long could it take?  Jay Mama: We are ready  Pranu: ðŸ¤£ðŸ¤£  Jay Mama: Call me pranu  Jay Mama: When your free  Pranu: This is a lengthy review and neither me has gone through it but seems like a good one from the views and the guy whoâ€™s giving it! Anyways, Iâ€™m hell bent on to XBOX cause I fell for the Game â€œForza horizon 4â€ anyways have a look Jay mama! And state your opinion!!  Jay Mama: Oh super will check our pranu  Jay Mama: Let me check other reviews  Jay Mama: As well  Pranu: Ya sure Jay mama!  Jay Mama: Donâ€™t post that mass video ra  Jay Mama: Itâ€™s funny ðŸ˜„  Pranu: This review was made much before the release of either of these consoles but it was pretty much right about Hardware specs! So might help Jay mama!!  Pranu: Jay mama did u review about buying the XBOX?  Pranu: Iâ€™m just a so very curious to have it asap  Jay Mama: All sold out pranu  Jay Mama: Waiting for to be back in stock  Jay Mama: Is it available in India ?  Pranu: Ya Jay mama like on Amazon  Jay Mama: Really  Jay Mama: How much ?  Jay Mama: Is it officially launched in India  Pranu: Oh wait  Pranu: Out it stick Jay mama  Pranu: But ya itâ€™s officially released and costs  Pranu: 49,900  Pranu: *out of stock  Jay Mama: In USA 37,000  Jay Mama: 12k difference  Jay Mama: But stock is not there  Jay Mama: Our today discussion in video. Watch it when free pranu  Pranu: That was a great talk Jay mama. This is really really gone shape the way I think whenever I think of innovations.  Pranu: Iâ€™ll keep following his talks now on  Pranu: This is our elective course offering Jay mama itâ€™s not available to us now but anyway would be available in third year  Pranu: These are all the minors that are being offered this Sem  Jay Mama: Sure Pranu will checkout and let you know by Sunday  Pranu: Ya jay mama no probs  Pranu: But just be known that the deadline for this registration is 31.1.2021 and itâ€™s limited seats  Jay Mama: This is really interesting course pranu. Take it whenever available  Jay Mama: AI & ML course feels valuable. What u think nana ?  Pranu: Ya but this is for other departments other than CSE cause we will get this in the main stream in third year  Pranu: So this would be available in third year Jay mama as it is an elective so Iâ€™ll pick it when offered to me  Pranu: So Jay mama I feel for this sem I could just start an online psychology course and let go the minors. And in the third sem as you said I could pick the IDEA 104 course as elective  Jay Mama: Ok ðŸ‘ðŸ»  Pranu: Because taking any minor now could burden the work of taking that interesting IDEA 104 course in next year  Pranu: Because minor lasts for 4 years  Jay Mama: Oh nice , Do whatever u feel interesting u know better than anyone  Pranu: True ok then this sem a self learning online Yale university psychology course and next year the BBA elective  Pranu: Watch this on Amazon Prime Jay mama!! Really a great One!  Jay Mama: Ok will do  Jay Mama: I would say to wait pranu.  Jay Mama: We donâ€™t if it real of products have been replaced or not ?  Pranu: Ya sure Jay mama  Pranu: And also I got to know thatâ€™s Xbox series X  Pranu: Is getting available  Pranu: On Amazon  Pranu: Soon  Pranu: Like from April  Jay Mama: U shall get in a month or 2.  Jay Mama: Iâ€™m planning to India in June  Pranu: Ya thatâ€™s what! Just to make an enquiry of reliability of this website I posted the image  Pranu: Great News!  Pranu: Jay mama just now while chatting with my friends, it just came up that you pay for my Netflix account! And they were like â€œur uncle is super cool!! Netflix is costly and itâ€™s super cool that you are enjoying Netflix for free because of your uncleâ€  Pranu: Thatâ€™s really cool Jay mama! ðŸ˜ŽðŸ˜ŽItâ€™s not just me who enjoyed but also shared it to my other friends for free, all thanks to you!  Pranu: ðŸ¤£ðŸ¤£  Pranu: #GifKing  Jay Mama: X box series x Kada pranu  Pranu: Ejactly  Pranu: Jay mama  Pranu: Hmm â€œout of stockâ€!! in a very great demand  Jay Mama: They are not making enough products. Donâ€™t know why  Pranu: I gotta a news that soon itâ€™s going to be available on Amazon  Pranu: Something like from April  Jay Mama: Letâ€™s wait and see PlayStation is available  Pranu: It seems that people with computers and tech knowledge are using Bots to buy Xbox soon after they get into stores!! Since bots are faster than humans they are making transactions faster and these tech guys are later selling the products for double the price  Pranu: Ya but Jay mama thereâ€™s actually nothing good to play on play station like I would like to play Forza, red dead redemption and games like that they are top notch  Pranu: And Xbox series S is that we have to pay subscription so thatâ€™s gonna cost us more than series X  Pranu: In two to three years  Pranu: Which could be played on Xbox alone By the way  Jay Mama: Oh no ðŸ¤¦\u200dâ™‚ï¸ Inka wait cheyyaddam. Thappa we canâ€™t do anything Kada  Pranu: Yaa no probs seems like series x gonna come soon  Pranu: I waited this long Iâ€™m ready to wait a couple more months  Pranu: Because even if I get Series S or play station I wonâ€™t be satisfied  Pranu: Available on Amazon Prime  Jay Mama: Nice review   Pranu: Ya really a nice one  Jay Mama: How US college admissions works ? Seen â€œOperation Varsity Blues:\xa0The\xa0College\xa0Admissions\xa0Scandalâ€ on Netflix yet?  Pranu: Watched the trailer Jay mama seems pretty much interesting will give it a watch  Pranu: For sure  Jay Mama: So funny pranu  Jay Mama: Similar to somebody , nobody jokes  Pranu: Ya ðŸ˜†ðŸ˜†  Pranu: Whoâ€™s not gonna like it?  Pranu: Jay mama suggest me a power bank for the phone I badly need one!  Pranu: Especially with a lower battery capacity for the iPhone 7  Jay Mama: What happened any problem with phone pranu  Pranu: Generally the battery of iPhone 7 is less right it gets completed soon and also recently while taking exam the power was gone and I had 7% charge on the phone so I felt the need of a power bank and also that would be helpful when I go to college Jay mama  Pranu: So even amma was asking for a power bank without need to carry a charger   Jay Mama: Ok buy 10000 or 15000 mah power bank  Pranu: Ya thought the same Jay mama! So MI power bank would be nice right?  Jay Mama: Yes ðŸ‘ðŸ»  Jay Mama: All are same  Pranu: Ohh ok  Jay Mama: Eey killer DP pic ðŸ‘ðŸ»ðŸ‘ðŸ»ðŸ‘ðŸ»  Pranu: Thank you Jay mama"""


def edit_distance(text1, text2):
  text1 = "$"+text1
  text2 = "$"+text2

  D = np.zeros((len(text1), len(text2)), dtype=int)

  count = 0
  for indr in range(D.shape[0]):
    D[indr, 0] = count
    count += 1
  
  count = 0
  for indc in range(D.shape[1]):
    D[0, indc] = count
    count += 1
  
  for indr in range(1,D.shape[0]):
    for indc in range(1,D.shape[1]):
      min_err_move = min([D[indr-1, indc-1] +1*int(text1[indr]!=text2[indc]), D[indr-1, indc]+1, D[indr, indc-1]+1, D[indr-2, indc-2]+1+1000*((text1[indr]!=text2[indc-1]) or (text1[indr-1]!=text2[indc])) if indr-2 >=0 and indc-2 >=0 else 100000]) #Replace, Deletion, Insertion, Transpose
      D[indr, indc] = min_err_move
  return D[-1, -1]



def make_edits(vocab, within_edits=2):
  def one_edits(word):
    edits = set()
    for i in range(len(word)):
      stage = list(word)
      stage.pop(i)
      edits.add("".join(stage))
    return edits
  
  token_edits = {k:[] for k in vocab.keys()}
  for word in vocab:
    source = set([word])
    for epoch in range(within_edits):
      new_source = set()
      for wdrep in source:
        temp_edits = one_edits(wdrep)
        new_source = new_source.union(temp_edits)
      source = new_source
      token_edits[word].append(list(new_source))
  return token_edits

def suggest_correct_words(raw_word, vocab_edits, topn=1):
  raw_word_edits = make_edits({raw_word:0}, within_edits=1)[raw_word][0]
  suggestions = []
  
  used_vocab = {vocab:0 for vocab in vocab_edits}
  suggestion_count = 0
  for rwe in raw_word_edits:
    for vocab, ves in vocab_edits.items():
      for dis, ve in enumerate(ves):
        if rwe in ve and not(used_vocab[vocab]):
          suggestions.append((vocab, dis+1))
          suggestion_count += 1
          used_vocab[vocab] = 1
          if suggestion_count >=topn:
            return suggestions
          break
  return suggestions
      



  pass
#edit_distance("htta", "that")

text_on_topic = wikipedia.page(title="Game").content.lower()

"""**Text-Preprocessing : Tokenization, Spelling Correction, Stemming, Lemmatization**"""

from collections import defaultdict as dd
from copy import deepcopy


class TextProcessing:
  LISTOFCORRECTWORDS =    """a<br>abandon<br>ability<br>able<br>abortion<br>about<br>above<br>abroad<br>absence<br>absolute<br>absolutely<br>absorb<br>abuse<br>academic<br>accept<br>access<br>accident<br>accompany<br>accomplish<br>according<br>account<br>accurate<br>accuse<br>achieve<br>achievement<br>acid<br>acknowledge<br>acquire<br>across<br>act<br>action<br>active<br>activist<br>activity<br>actor<br>actress<br>actual<br>actually<br>ad<br>adapt<br>add<br>addition<br>additional<br>address<br>adequate<br>adjust<br>adjustment<br>administration<br>administrator<br>admire<br>admission<br>admit<br>adolescent<br>adopt<br>adult<br>advance<br>advanced<br>advantage<br>adventure<br>advertising<br>advice<br>advise<br>adviser<br>advocate<br>affair<br>affect<br>afford<br>afraid<br>African<br>African-American<br>after<br>afternoon<br>again<br>against<br>age<br>agency<br>agenda<br>agent<br>aggressive<br>ago<br>agree<br>agreement<br>agricultural<br>ah<br>ahead<br>aid<br>aide<br>AIDS<br>aim<br>air<br>aircraft<br>airline<br>airport<br>album<br>alcohol<br>alive<br>all<br>alliance<br>allow<br>ally<br>almost<br>alone<br>along<br>already<br>also<br>alter<br>alternative<br>although<br>always<br>AM<br>amazing<br>American<br>among<br>amount<br>analysis<br>analyst<br>analyze<br>ancient<br>and<br>anger<br>angle<br>angry<br>animal<br>anniversary<br>announce<br>annual<br>another<br>answer<br>anticipate<br>anxiety<br>any<br>anybody<br>anymore<br>anyone<br>anything<br>anyway<br>anywhere<br>apart<br>apartment<br>apparent<br>apparently<br>appeal<br>appear<br>appearance<br>apple<br>application<br>apply<br>appoint<br>appointment<br>appreciate<br>approach<br>appropriate<br>approval<br>approve<br>approximately<br>Arab<br>architect<br>area<br>argue<br>argument<br>arise<br>arm<br>armed<br>army<br>around<br>arrange<br>arrangement<br>arrest<br>arrival<br>arrive<br>art<br>article<br>artist<br>artistic<br>as<br>Asian<br>aside<br>ask<br>asleep<br>aspect<br>assault<br>assert<br>assess<br>assessment<br>asset<br>assign<br>assignment<br>assist<br>assistance<br>assistant<br>associate<br>association<br>assume<br>assumption<br>assure<br>at<br>athlete<br>athletic<br>atmosphere<br>attach<br>attack<br>attempt<br>attend<br>attention<br>attitude<br>attorney<br>attract<br>attractive<br>attribute<br>audience<br>author<br>authority<br>auto<br>available<br>average<br>avoid<br>award<br>aware<br>awareness<br>away<br>awful<br>baby<br>back<br>background<br>bad<br>badly<br>bag<br>bake<br>balance<br>ball<br>ban<br>band<br>bank<br>bar<br>barely<br>barrel<br>barrier<br>base<br>baseball<br>basic<br>basically<br>basis<br>basket<br>basketball<br>bathroom<br>battery<br>battle<br>be<br>beach<br>bean<br>bear<br>beat<br>beautiful<br>beauty<br>because<br>become<br>bed<br>bedroom<br>beer<br>before<br>begin<br>beginning<br>behavior<br>behind<br>being<br>belief<br>believe<br>bell<br>belong<br>below<br>belt<br>bench<br>bend<br>beneath<br>benefit<br>beside<br>besides<br>best<br>bet<br>better<br>between<br>beyond<br>Bible<br>big<br>bike<br>bill<br>billion<br>bind<br>biological<br>bird<br>birth<br>birthday<br>bit<br>bite<br>black<br>blade<br>blame<br>blanket<br>blind<br>block<br>blood<br>blow<br>blue<br>board<br>boat<br>body<br>bomb<br>bombing<br>bond<br>bone<br>book<br>boom<br>boot<br>border<br>born<br>borrow<br>boss<br>both<br>bother<br>bottle<br>bottom<br>boundary<br>bowl<br>box<br>boy<br>boyfriend<br>brain<br>branch<br>brand<br>bread<br>break<br>breakfast<br>breast<br>breath<br>breathe<br>brick<br>bridge<br>brief<br>briefly<br>bright<br>brilliant<br>bring<br>British<br>broad<br>broken<br>brother<br>brown<br>brush<br>buck<br>budget<br>build<br>building<br>bullet<br>bunch<br>burden<br>burn<br>bury<br>bus<br>business<br>busy<br>but<br>butter<br>button<br>buy<br>buyer<br>by<br>cabin<br>cabinet<br>cable<br>cake<br>calculate<br>call<br>camera<br>camp<br>campaign<br>campus<br>can<br>Canadian<br>cancer<br>candidate<br>cap<br>capability<br>capable<br>capacity<br>capital<br>captain<br>capture<br>car<br>carbon<br>card<br>care<br>career<br>careful<br>carefully<br>carrier<br>carry<br>case<br>cash<br>cast<br>cat<br>catch<br>category<br>Catholic<br>cause<br>ceiling<br>celebrate<br>celebration<br>celebrity<br>cell<br>center<br>central<br>century<br>CEO<br>ceremony<br>certain<br>certainly<br>chain<br>chair<br>chairman<br>challenge<br>chamber<br>champion<br>championship<br>chance<br>change<br>changing<br>channel<br>chapter<br>character<br>characteristic<br>characterize<br>charge<br>charity<br>chart<br>chase<br>cheap<br>check<br>cheek<br>cheese<br>chef<br>chemical<br>chest<br>chicken<br>chief<br>child<br>childhood<br>Chinese<br>chip<br>chocolate<br>choice<br>cholesterol<br>choose<br>Christian<br>Christmas<br>church<br>cigarette<br>circle<br>circumstance<br>cite<br>citizen<br>city<br>civil<br>civilian<br>claim<br>class<br>classic<br>classroom<br>clean<br>clear<br>clearly<br>client<br>climate<br>climb<br>clinic<br>clinical<br>clock<br>close<br>closely<br>closer<br>clothes<br>clothing<br>cloud<br>club<br>clue<br>cluster<br>coach<br>coal<br>coalition<br>coast<br>coat<br>code<br>coffee<br>cognitive<br>cold<br>collapse<br>colleague<br>collect<br>collection<br>collective<br>college<br>colonial<br>color<br>column<br>combination<br>combine<br>come<br>comedy<br>comfort<br>comfortable<br>command<br>commander<br>comment<br>commercial<br>commission<br>commit<br>commitment<br>committee<br>common<br>communicate<br>communication<br>community<br>company<br>compare<br>comparison<br>compete<br>competition<br>competitive<br>competitor<br>complain<br>complaint<br>complete<br>completely<br>complex<br>complicated<br>component<br>compose<br>composition<br>comprehensive<br>computer<br>concentrate<br>concentration<br>concept<br>concern<br>concerned<br>concert<br>conclude<br>conclusion<br>concrete<br>condition<br>conduct<br>conference<br>confidence<br>confident<br>confirm<br>conflict<br>confront<br>confusion<br>Congress<br>congressional<br>connect<br>connection<br>consciousness<br>consensus<br>consequence<br>conservative<br>consider<br>considerable<br>consideration<br>consist<br>consistent<br>constant<br>constantly<br>constitute<br>constitutional<br>construct<br>construction<br>consultant<br>consume<br>consumer<br>consumption<br>contact<br>contain<br>container<br>contemporary<br>content<br>contest<br>context<br>continue<br>continued<br>contract<br>contrast<br>contribute<br>contribution<br>control<br>controversial<br>controversy<br>convention<br>conventional<br>conversation<br>convert<br>conviction<br>convince<br>cook<br>cookie<br>cooking<br>cool<br>cooperation<br>cop<br>cope<br>copy<br>core<br>corn<br>corner<br>corporate<br>corporation<br>correct<br>correspondent<br>cost<br>cotton<br>couch<br>could<br>council<br>counselor<br>count<br>counter<br>country<br>county<br>couple<br>courage<br>course<br>court<br>cousin<br>cover<br>coverage<br>cow<br>crack<br>craft<br>crash<br>crazy<br>cream<br>create<br>creation<br>creative<br>creature<br>credit<br>crew<br>crime<br>criminal<br>crisis<br>criteria<br>critic<br>critical<br>criticism<br>criticize<br>crop<br>cross<br>crowd<br>crucial<br>cry<br>cultural<br>culture<br>cup<br>curious<br>current<br>currently<br>curriculum<br>custom<br>customer<br>cut<br>cycle<br>dad<br>daily<br>damage<br>dance<br>danger<br>dangerous<br>dare<br>dark<br>darkness<br>data<br>date<br>daughter<br>day<br>dead<br>deal<br>dealer<br>dear<br>death<br>debate<br>debt<br>decade<br>decide<br>decision<br>deck<br>declare<br>decline<br>decrease<br>deep<br>deeply<br>deer<br>defeat<br>defend<br>defendant<br>defense<br>defensive<br>deficit<br>define<br>definitely<br>definition<br>degree<br>delay<br>deliver<br>delivery<br>demand<br>democracy<br>Democrat<br>democratic<br>demonstrate<br>demonstration<br>deny<br>department<br>depend<br>dependent<br>depending<br>depict<br>depression<br>depth<br>deputy<br>derive<br>describe<br>description<br>desert<br>deserve<br>design<br>designer<br>desire<br>desk<br>desperate<br>despite<br>destroy<br>destruction<br>detail<br>detailed<br>detect<br>determine<br>develop<br>developing<br>development<br>device<br>devote<br>dialogue<br>die<br>diet<br>differ<br>difference<br>different<br>differently<br>difficult<br>difficulty<br>dig<br>digital<br>dimension<br>dining<br>dinner<br>direct<br>direction<br>directly<br>director<br>dirt<br>dirty<br>disability<br>disagree<br>disappear<br>disaster<br>discipline<br>discourse<br>discover<br>discovery<br>discrimination<br>discuss<br>discussion<br>disease<br>dish<br>dismiss<br>disorder<br>display<br>dispute<br>distance<br>distant<br>distinct<br>distinction<br>distinguish<br>distribute<br>distribution<br>district<br>diverse<br>diversity<br>divide<br>division<br>divorce<br>DNA<br>do<br>doctor<br>document<br>dog<br>domestic<br>dominant<br>dominate<br>door<br>double<br>doubt<br>down<br>downtown<br>dozen<br>draft<br>drag<br>drama<br>dramatic<br>dramatically<br>draw<br>drawing<br>dream<br>dress<br>drink<br>drive<br>driver<br>drop<br>drug<br>dry<br>due<br>during<br>dust<br>duty<br>each<br>eager<br>ear<br>early<br>earn<br>earnings<br>earth<br>ease<br>easily<br>east<br>eastern<br>easy<br>eat<br>economic<br>economics<br>economist<br>economy<br>edge<br>edition<br>editor<br>educate<br>education<br>educational<br>educator<br>effect<br>effective<br>effectively<br>efficiency<br>efficient<br>effort<br>egg<br>eight<br>either<br>elderly<br>elect<br>election<br>electric<br>electricity<br>electronic<br>element<br>elementary<br>eliminate<br>elite<br>else<br>elsewhere<br>e-mail<br>embrace<br>emerge<br>emergency<br>emission<br>emotion<br>emotional<br>emphasis<br>emphasize<br>employ<br>employee<br>employer<br>employment<br>empty<br>enable<br>encounter<br>encourage<br>end<br>enemy<br>energy<br>enforcement<br>engage<br>engine<br>engineer<br>engineering<br>English<br>enhance<br>enjoy<br>enormous<br>enough<br>ensure<br>enter<br>enterprise<br>entertainment<br>entire<br>entirely<br>entrance<br>entry<br>environment<br>environmental<br>episode<br>equal<br>equally<br>equipment<br>era<br>error<br>escape<br>especially<br>essay<br>essential<br>essentially<br>establish<br>establishment<br>estate<br>estimate<br>etc<br>ethics<br>ethnic<br>European<br>evaluate<br>evaluation<br>even<br>evening<br>event<br>eventually<br>ever<br>every<br>everybody<br>everyday<br>everyone<br>everything<br>everywhere<br>evidence<br>evolution<br>evolve<br>exact<br>exactly<br>examination<br>examine<br>example<br>exceed<br>excellent<br>except<br>exception<br>exchange<br>exciting<br>executive<br>exercise<br>exhibit<br>exhibition<br>exist<br>existence<br>existing<br>expand<br>expansion<br>expect<br>expectation<br>expense<br>expensive<br>experience<br>experiment<br>expert<br>explain<br>explanation<br>explode<br>explore<br>explosion<br>expose<br>exposure<br>express<br>expression<br>extend<br>extension<br>extensive<br>extent<br>external<br>extra<br>extraordinary<br>extreme<br>extremely<br>eye<br>fabric<br>face<br>facility<br>fact<br>factor<br>factory<br>faculty<br>fade<br>fail<br>failure<br>fair<br>fairly<br>faith<br>fall<br>false<br>familiar<br>family<br>famous<br>fan<br>fantasy<br>far<br>farm<br>farmer<br>fashion<br>fast<br>fat<br>fate<br>father<br>fault<br>favor<br>favorite<br>fear<br>feature<br>federal<br>fee<br>feed<br>feel<br>feeling<br>fellow<br>female<br>fence<br>few<br>fewer<br>fiber<br>fiction<br>field<br>fifteen<br>fifth<br>fifty<br>fight<br>fighter<br>fighting<br>figure<br>file<br>fill<br>film<br>final<br>finally<br>finance<br>financial<br>find<br>finding<br>fine<br>finger<br>finish<br>fire<br>firm<br>first<br>fish<br>fishing<br>fit<br>fitness<br>five<br>fix<br>flag<br>flame<br>flat<br>flavor<br>flee<br>flesh<br>flight<br>float<br>floor<br>flow<br>flower<br>fly<br>focus<br>folk<br>follow<br>following<br>food<br>foot<br>football<br>for<br>force<br>foreign<br>forest<br>forever<br>forget<br>form<br>formal<br>formation<br>former<br>formula<br>forth<br>fortune<br>forward<br>found<br>foundation<br>founder<br>four<br>fourth<br>frame<br>framework<br>free<br>freedom<br>freeze<br>French<br>frequency<br>frequent<br>frequently<br>fresh<br>friend<br>friendly<br>friendship<br>from<br>front<br>fruit<br>frustration<br>fuel<br>full<br>fully<br>fun<br>function<br>fund<br>fundamental<br>funding<br>funeral<br>funny<br>furniture<br>furthermore<br>future<br>gain<br>galaxy<br>gallery<br>game<br>gang<br>gap<br>garage<br>garden<br>garlic<br>gas<br>gate<br>gather<br>gay<br>gaze<br>gear<br>gender<br>gene<br>general<br>generally<br>generate<br>generation<br>genetic<br>gentleman<br>gently<br>German<br>gesture<br>get<br>ghost<br>giant<br>gift<br>gifted<br>girl<br>girlfriend<br>give<br>given<br>glad<br>glance<br>glass<br>global<br>glove<br>go<br>goal<br>God<br>gold<br>golden<br>golf<br>good<br>government<br>governor<br>grab<br>grade<br>gradually<br>graduate<br>grain<br>grand<br>grandfather<br>grandmother<br>grant<br>grass<br>grave<br>gray<br>great<br>greatest<br>green<br>grocery<br>ground<br>group<br>grow<br>growing<br>growth<br>guarantee<br>guard<br>guess<br>guest<br>guide<br>guideline<br>guilty<br>gun<br>guy<br>habit<br>habitat<br>hair<br>half<br>hall<br>hand<br>handful<br>handle<br>hang<br>happen<br>happy<br>hard<br>hardly<br>hat<br>hate<br>have<br>he<br>head<br>headline<br>headquarters<br>health<br>healthy<br>hear<br>hearing<br>heart<br>heat<br>heaven<br>heavily<br>heavy<br>heel<br>height<br>helicopter<br>hell<br>hello<br>help<br>helpful<br>her<br>here<br>heritage<br>hero<br>herself<br>hey<br>hi<br>hide<br>high<br>highlight<br>highly<br>highway<br>hill<br>him<br>himself<br>hip<br>hire<br>his<br>historian<br>historic<br>historical<br>history<br>hit<br>hold<br>hole<br>holiday<br>holy<br>home<br>homeless<br>honest<br>honey<br>honor<br>hope<br>horizon<br>horror<br>horse<br>hospital<br>host<br>hot<br>hotel<br>hour<br>house<br>household<br>housing<br>how<br>however<br>huge<br>human<br>humor<br>hundred<br>hungry<br>hunter<br>hunting<br>hurt<br>husband<br>hypothesis<br>I<br>ice<br>idea<br>ideal<br>identification<br>identify<br>identity<br>ie<br>if<br>ignore<br>ill<br>illegal<br>illness<br>illustrate<br>image<br>imagination<br>imagine<br>immediate<br>immediately<br>immigrant<br>immigration<br>impact<br>implement<br>implication<br>imply<br>importance<br>important<br>impose<br>impossible<br>impress<br>impression<br>impressive<br>improve<br>improvement<br>in<br>incentive<br>incident<br>include<br>including<br>income<br>incorporate<br>increase<br>increased<br>increasing<br>increasingly<br>incredible<br>indeed<br>independence<br>independent<br>index<br>Indian<br>indicate<br>indication<br>individual<br>industrial<br>industry<br>infant<br>infection<br>inflation<br>influence<br>inform<br>information<br>ingredient<br>initial<br>initially<br>initiative<br>injury<br>inner<br>innocent<br>inquiry<br>inside<br>insight<br>insist<br>inspire<br>install<br>instance<br>instead<br>institution<br>institutional<br>instruction<br>instructor<br>instrument<br>insurance<br>intellectual<br>intelligence<br>intend<br>intense<br>intensity<br>intention<br>interaction<br>interest<br>interested<br>interesting<br>internal<br>international<br>Internet<br>interpret<br>interpretation<br>intervention<br>interview<br>into<br>introduce<br>introduction<br>invasion<br>invest<br>investigate<br>investigation<br>investigator<br>investment<br>investor<br>invite<br>involve<br>involved<br>involvement<br>Iraqi<br>Irish<br>iron<br>Islamic<br>island<br>Israeli<br>issue<br>it<br>Italian<br>item<br>its<br>itself<br>jacket<br>jail<br>Japanese<br>jet<br>Jew<br>Jewish<br>job<br>join<br>joint<br>joke<br>journal<br>journalist<br>journey<br>joy<br>judge<br>judgment<br>juice<br>jump<br>junior<br>jury<br>just<br>justice<br>justify<br>keep<br>key<br>kick<br>kid<br>kill<br>killer<br>killing<br>kind<br>king<br>kiss<br>kitchen<br>knee<br>knife<br>knock<br>know<br>knowledge<br>lab<br>label<br>labor<br>laboratory<br>lack<br>lady<br>lake<br>land<br>landscape<br>language<br>lap<br>large<br>largely<br>last<br>late<br>later<br>Latin<br>latter<br>laugh<br>launch<br>law<br>lawn<br>lawsuit<br>lawyer<br>lay<br>layer<br>lead<br>leader<br>leadership<br>leading<br>leaf<br>league<br>lean<br>learn<br>learning<br>least<br>leather<br>leave<br>left<br>leg<br>legacy<br>legal<br>legend<br>legislation<br>legitimate<br>lemon<br>length<br>less<br>lesson<br>let<br>letter<br>level<br>liberal<br>library<br>license<br>lie<br>life<br>lifestyle<br>lifetime<br>lift<br>light<br>like<br>likely<br>limit<br>limitation<br>limited<br>line<br>link<br>lip<br>list<br>listen<br>literally<br>literary<br>literature<br>little<br>live<br>living<br>load<br>loan<br>local<br>locate<br>location<br>lock<br>long<br>long-term<br>look<br>loose<br>lose<br>loss<br>lost<br>lot<br>lots<br>loud<br>love<br>lovely<br>lover<br>low<br>lower<br>luck<br>lucky<br>lunch<br>lung<br>machine<br>mad<br>magazine<br>mail<br>main<br>mainly<br>maintain<br>maintenance<br>major<br>majority<br>make<br>maker<br>makeup<br>male<br>mall<br>man<br>manage<br>management<br>manager<br>manner<br>manufacturer<br>manufacturing<br>many<br>map<br>margin<br>mark<br>market<br>marketing<br>marriage<br>married<br>marry<br>mask<br>mass<br>massive<br>master<br>match<br>material<br>math<br>matter<br>may<br>maybe<br>mayor<br>me<br>meal<br>mean<br>meaning<br>meanwhile<br>measure<br>measurement<br>meat<br>mechanism<br>media<br>medical<br>medication<br>medicine<br>medium<br>meet<br>meeting<br>member<br>membership<br>memory<br>mental<br>mention<br>menu<br>mere<br>merely<br>mess<br>message<br>metal<br>meter<br>method<br>Mexican<br>middle<br>might<br>military<br>milk<br>million<br>mind<br>mine<br>minister<br>minor<br>minority<br>minute<br>miracle<br>mirror<br>miss<br>missile<br>mission<br>mistake<br>mix<br>mixture<br>mm-hmm<br>mode<br>model<br>moderate<br>modern<br>modest<br>mom<br>moment<br>money<br>monitor<br>month<br>mood<br>moon<br>moral<br>more<br>moreover<br>morning<br>mortgage<br>most<br>mostly<br>mother<br>motion<br>motivation<br>motor<br>mount<br>mountain<br>mouse<br>mouth<br>move<br>movement<br>movie<br>Mr<br>Mrs<br>Ms<br>much<br>multiple<br>murder<br>muscle<br>museum<br>music<br>musical<br>musician<br>Muslim<br>must<br>mutual<br>my<br>myself<br>mystery<br>myth<br>naked<br>name<br>narrative<br>narrow<br>nation<br>national<br>native<br>natural<br>naturally<br>nature<br>near<br>nearby<br>nearly<br>necessarily<br>necessary<br>neck<br>need<br>negative<br>negotiate<br>negotiation<br>neighbor<br>neighborhood<br>neither<br>nerve<br>nervous<br>net<br>network<br>never<br>nevertheless<br>new<br>newly<br>news<br>newspaper<br>next<br>nice<br>night<br>nine<br>no<br>nobody<br>nod<br>noise<br>nomination<br>none<br>nonetheless<br>nor<br>normal<br>normally<br>north<br>northern<br>nose<br>not<br>note<br>nothing<br>notice<br>notion<br>novel<br>now<br>nowhere<br>n't<br>nuclear<br>number<br>numerous<br>nurse<br>nut<br>object<br>objective<br>obligation<br>observation<br>observe<br>observer<br>obtain<br>obvious<br>obviously<br>occasion<br>occasionally<br>occupation<br>occupy<br>occur<br>ocean<br>odd<br>odds<br>of<br>off<br>offense<br>offensive<br>offer<br>office<br>officer<br>official<br>often<br>oh<br>oil<br>ok<br>okay<br>old<br>Olympic<br>on<br>once<br>one<br>ongoing<br>onion<br>online<br>only<br>onto<br>open<br>opening<br>operate<br>operating<br>operation<br>operator<br>opinion<br>opponent<br>opportunity<br>oppose<br>opposite<br>opposition<br>option<br>or<br>orange<br>order<br>ordinary<br>organic<br>organization<br>organize<br>orientation<br>origin<br>original<br>originally<br>other<br>others<br>otherwise<br>ought<br>our<br>ourselves<br>out<br>outcome<br>outside<br>oven<br>over<br>overall<br>overcome<br>overlook<br>owe<br>own<br>owner<br>pace<br>pack<br>package<br>page<br>pain<br>painful<br>paint<br>painter<br>painting<br>pair<br>pale<br>Palestinian<br>palm<br>pan<br>panel<br>pant<br>paper<br>parent<br>park<br>parking<br>part<br>participant<br>participate<br>participation<br>particular<br>particularly<br>partly<br>partner<br>partnership<br>party<br>pass<br>passage<br>passenger<br>passion<br>past<br>patch<br>path<br>patient<br>pattern<br>pause<br>pay<br>payment<br>PC<br>peace<br>peak<br>peer<br>penalty<br>people<br>pepper<br>per<br>perceive<br>percentage<br>perception<br>perfect<br>perfectly<br>perform<br>performance<br>perhaps<br>period<br>permanent<br>permission<br>permit<br>person<br>personal<br>personality<br>personally<br>personnel<br>perspective<br>persuade<br>pet<br>phase<br>phenomenon<br>philosophy<br>phone<br>photo<br>photograph<br>photographer<br>phrase<br>physical<br>physically<br>physician<br>piano<br>pick<br>picture<br>pie<br>piece<br>pile<br>pilot<br>pine<br>pink<br>pipe<br>pitch<br>place<br>plan<br>plane<br>planet<br>planning<br>plant<br>plastic<br>plate<br>platform<br>play<br>player<br>please<br>pleasure<br>plenty<br>plot<br>plus<br>PM<br>pocket<br>poem<br>poet<br>poetry<br>point<br>pole<br>police<br>policy<br>political<br>politically<br>politician<br>politics<br>poll<br>pollution<br>pool<br>poor<br>pop<br>popular<br>population<br>porch<br>port<br>portion<br>portrait<br>portray<br>pose<br>position<br>positive<br>possess<br>possibility<br>possible<br>possibly<br>post<br>pot<br>potato<br>potential<br>potentially<br>pound<br>pour<br>poverty<br>powder<br>power<br>powerful<br>practical<br>practice<br>pray<br>prayer<br>precisely<br>predict<br>prefer<br>preference<br>pregnancy<br>pregnant<br>preparation<br>prepare<br>prescription<br>presence<br>present<br>presentation<br>preserve<br>president<br>presidential<br>press<br>pressure<br>pretend<br>pretty<br>prevent<br>previous<br>previously<br>price<br>pride<br>priest<br>primarily<br>primary<br>prime<br>principal<br>principle<br>print<br>prior<br>priority<br>prison<br>prisoner<br>privacy<br>private<br>probably<br>problem<br>procedure<br>proceed<br>process<br>produce<br>producer<br>product<br>production<br>profession<br>professional<br>professor<br>profile<br>profit<br>program<br>progress<br>project<br>prominent<br>promise<br>promote<br>prompt<br>proof<br>proper<br>properly<br>property<br>proportion<br>proposal<br>propose<br>proposed<br>prosecutor<br>prospect<br>protect<br>protection<br>protein<br>protest<br>proud<br>prove<br>provide<br>provider<br>province<br>provision<br>psychological<br>psychologist<br>psychology<br>public<br>publication<br>publicly<br>publish<br>publisher<br>pull<br>punishment<br>purchase<br>pure<br>purpose<br>pursue<br>push<br>put<br>qualify<br>quality<br>quarter<br>quarterback<br>question<br>quick<br>quickly<br>quiet<br>quietly<br>quit<br>quite<br>quote<br>race<br>racial<br>radical<br>radio<br>rail<br>rain<br>raise<br>range<br>rank<br>rapid<br>rapidly<br>rare<br>rarely<br>rate<br>rather<br>rating<br>ratio<br>raw<br>reach<br>react<br>reaction<br>read<br>reader<br>reading<br>ready<br>real<br>reality<br>realize<br>really<br>reason<br>reasonable<br>recall<br>receive<br>recent<br>recently<br>recipe<br>recognition<br>recognize<br>recommend<br>recommendation<br>record<br>recording<br>recover<br>recovery<br>recruit<br>red<br>reduce<br>reduction<br>refer<br>reference<br>reflect<br>reflection<br>reform<br>refugee<br>refuse<br>regard<br>regarding<br>regardless<br>regime<br>region<br>regional<br>register<br>regular<br>regularly<br>regulate<br>regulation<br>reinforce<br>reject<br>relate<br>relation<br>relationship<br>relative<br>relatively<br>relax<br>release<br>relevant<br>relief<br>religion<br>religious<br>rely<br>remain<br>remaining<br>remarkable<br>remember<br>remind<br>remote<br>remove<br>repeat<br>repeatedly<br>replace<br>reply<br>report<br>reporter<br>represent<br>representation<br>representative<br>Republican<br>reputation<br>request<br>require<br>requirement<br>research<br>researcher<br>resemble<br>reservation<br>resident<br>resist<br>resistance<br>resolution<br>resolve<br>resort<br>resource<br>respect<br>respond<br>respondent<br>response<br>responsibility<br>responsible<br>rest<br>restaurant<br>restore<br>restriction<br>result<br>retain<br>retire<br>retirement<br>return<br>reveal<br>revenue<br>review<br>revolution<br>rhythm<br>rice<br>rich<br>rid<br>ride<br>rifle<br>right<br>ring<br>rise<br>risk<br>river<br>road<br>rock<br>role<br>roll<br>romantic<br>roof<br>room<br>root<br>rope<br>rose<br>rough<br>roughly<br>round<br>route<br>routine<br>row<br>rub<br>rule<br>run<br>running<br>rural<br>rush<br>Russian<br>sacred<br>sad<br>safe<br>safety<br>sake<br>salad<br>salary<br>sale<br>sales<br>salt<br>same<br>sample<br>sanction<br>sand<br>satellite<br>satisfaction<br>satisfy<br>sauce<br>save<br>saving<br>say<br>scale<br>scandal<br>scared<br>scenario<br>scene<br>schedule<br>scheme<br>scholar<br>scholarship<br>school<br>science<br>scientific<br>scientist<br>scope<br>score<br>scream<br>screen<br>script<br>sea<br>search<br>season<br>seat<br>second<br>secret<br>secretary<br>section<br>sector<br>secure<br>security<br>see<br>seed<br>seek<br>seem<br>segment<br>seize<br>select<br>selection<br>self<br>sell<br>Senate<br>senator<br>send<br>senior<br>sense<br>sensitive<br>sentence<br>separate<br>sequence<br>series<br>serious<br>seriously<br>serve<br>service<br>session<br>set<br>setting<br>settle<br>settlement<br>seven<br>several<br>severe<br>sex<br>sexual<br>shade<br>shadow<br>shake<br>shall<br>shape<br>share<br>sharp<br>she<br>sheet<br>shelf<br>shell<br>shelter<br>shift<br>shine<br>ship<br>shirt<br>shit<br>shock<br>shoe<br>shoot<br>shooting<br>shop<br>shopping<br>shore<br>short<br>shortly<br>shot<br>should<br>shoulder<br>shout<br>show<br>shower<br>shrug<br>shut<br>sick<br>side<br>sigh<br>sight<br>sign<br>signal<br>significance<br>significant<br>significantly<br>silence<br>silent<br>silver<br>similar<br>similarly<br>simple<br>simply<br>sin<br>since<br>sing<br>singer<br>single<br>sink<br>sir<br>sister<br>sit<br>site<br>situation<br>six<br>size<br>ski<br>skill<br>skin<br>sky<br>slave<br>sleep<br>slice<br>slide<br>slight<br>slightly<br>slip<br>slow<br>slowly<br>small<br>smart<br>smell<br>smile<br>smoke<br>smooth<br>snap<br>snow<br>so<br>so-called<br>soccer<br>social<br>society<br>soft<br>software<br>soil<br>solar<br>soldier<br>solid<br>solution<br>solve<br>some<br>somebody<br>somehow<br>someone<br>something<br>sometimes<br>somewhat<br>somewhere<br>son<br>song<br>soon<br>sophisticated<br>sorry<br>sort<br>soul<br>sound<br>soup<br>source<br>south<br>southern<br>Soviet<br>space<br>Spanish<br>speak<br>speaker<br>special<br>specialist<br>species<br>specific<br>specifically<br>speech<br>speed<br>spend<br>spending<br>spin<br>spirit<br>spiritual<br>split<br>spokesman<br>sport<br>spot<br>spread<br>spring<br>square<br>squeeze<br>stability<br>stable<br>staff<br>stage<br>stair<br>stake<br>stand<br>standard<br>standing<br>star<br>stare<br>start<br>state<br>statement<br>station<br>statistics<br>status<br>stay<br>steady<br>steal<br>steel<br>step<br>stick<br>still<br>stir<br>stock<br>stomach<br>stone<br>stop<br>storage<br>store<br>storm<br>story<br>straight<br>strange<br>stranger<br>strategic<br>strategy<br>stream<br>street<br>strength<br>strengthen<br>stress<br>stretch<br>strike<br>string<br>strip<br>stroke<br>strong<br>strongly<br>structure<br>struggle<br>student<br>studio<br>study<br>stuff<br>stupid<br>style<br>subject<br>submit<br>subsequent<br>substance<br>substantial<br>succeed<br>success<br>successful<br>successfully<br>such<br>sudden<br>suddenly<br>sue<br>suffer<br>sufficient<br>sugar<br>suggest<br>suggestion<br>suicide<br>suit<br>summer<br>summit<br>sun<br>super<br>supply<br>support<br>supporter<br>suppose<br>supposed<br>Supreme<br>sure<br>surely<br>surface<br>surgery<br>surprise<br>surprised<br>surprising<br>surprisingly<br>surround<br>survey<br>survival<br>survive<br>survivor<br>suspect<br>sustain<br>swear<br>sweep<br>sweet<br>swim<br>swing<br>switch<br>symbol<br>symptom<br>system<br>table<br>tablespoon<br>tactic<br>tail<br>take<br>tale<br>talent<br>talk<br>tall<br>tank<br>tap<br>tape<br>target<br>task<br>taste<br>tax<br>taxpayer<br>tea<br>teach<br>teacher<br>teaching<br>team<br>tear<br>teaspoon<br>technical<br>technique<br>technology<br>teen<br>teenager<br>telephone<br>telescope<br>television<br>tell<br>temperature<br>temporary<br>ten<br>tend<br>tendency<br>tennis<br>tension<br>tent<br>term<br>terms<br>terrible<br>territory<br>terror<br>terrorism<br>terrorist<br>test<br>testify<br>testimony<br>testing<br>text<br>than<br>thank<br>thanks<br>that<br>the<br>theater<br>their<br>them<br>theme<br>themselves<br>then<br>theory<br>therapy<br>there<br>therefore<br>these<br>they<br>thick<br>thin<br>thing<br>think<br>thinking<br>third<br>thirty<br>this<br>those<br>though<br>thought<br>thousand<br>threat<br>threaten<br>three<br>throat<br>through<br>throughout<br>throw<br>thus<br>ticket<br>tie<br>tight<br>time<br>tiny<br>tip<br>tire<br>tired<br>tissue<br>title<br>to<br>tobacco<br>today<br>toe<br>together<br>tomato<br>tomorrow<br>tone<br>tongue<br>tonight<br>too<br>tool<br>tooth<br>top<br>topic<br>toss<br>total<br>totally<br>touch<br>tough<br>tour<br>tourist<br>tournament<br>toward<br>towards<br>tower<br>town<br>toy<br>trace<br>track<br>trade<br>tradition<br>traditional<br>traffic<br>tragedy<br>trail<br>train<br>training<br>transfer<br>transform<br>transformation<br>transition<br>translate<br>transportation<br>travel<br>treat<br>treatment<br>treaty<br>tree<br>tremendous<br>trend<br>trial<br>tribe<br>trick<br>trip<br>troop<br>trouble<br>truck<br>true<br>truly<br>trust<br>truth<br>try<br>tube<br>tunnel<br>turn<br>TV<br>twelve<br>twenty<br>twice<br>twin<br>two<br>type<br>typical<br>typically<br>ugly<br>ultimate<br>ultimately<br>unable<br>uncle<br>under<br>undergo<br>understand<br>understanding<br>unfortunately<br>uniform<br>union<br>unique<br>unit<br>United<br>universal<br>universe<br>university<br>unknown<br>unless<br>unlike<br>unlikely<br>until<br>unusual<br>up<br>upon<br>upper<br>urban<br>urge<br>us<br>use<br>used<br>useful<br>user<br>usual<br>usually<br>utility<br>vacation<br>valley<br>valuable<br>value<br>variable<br>variation<br>variety<br>various<br>vary<br>vast<br>vegetable<br>vehicle<br>venture<br>version<br>versus<br>very<br>vessel<br>veteran<br>via<br>victim<br>victory<br>video<br>view<br>viewer<br>village<br>violate<br>violation<br>violence<br>violent<br>virtually<br>virtue<br>virus<br>visible<br>vision<br>visit<br>visitor<br>visual<br>vital<br>voice<br>volume<br>volunteer<br>vote<br>voter<br>vs<br>vulnerable<br>wage<br>wait<br>wake<br>walk<br>wall<br>wander<br>want<br>war<br>warm<br>warn<br>warning<br>wash<br>waste<br>watch<br>water<br>wave<br>way<br>we<br>weak<br>wealth<br>wealthy<br>weapon<br>wear<br>weather<br>wedding<br>week<br>weekend<br>weekly<br>weigh<br>weight<br>welcome<br>welfare<br>well<br>west<br>western<br>wet<br>what<br>whatever<br>wheel<br>when<br>whenever<br>where<br>whereas<br>whether<br>which<br>while<br>whisper<br>white<br>who<br>whole<br>whom<br>whose<br>why<br>wide<br>widely<br>widespread<br>wife<br>wild<br>will<br>willing<br>win<br>wind<br>window<br>wine<br>wing<br>winner<br>winter<br>wipe<br>wire<br>wisdom<br>wise<br>wish<br>with<br>withdraw<br>within<br>without<br>witness<br>woman<br>wonder<br>wonderful<br>wood<br>wooden<br>word<br>work<br>worker<br>working<br>works<br>workshop<br>world<br>worried<br>worry<br>worth<br>would<br>wound<br>wrap<br>write<br>writer<br>writing<br>wrong<br>yard<br>yeah<br>year<br>yell<br>yellow<br>yes<br>yesterday<br>yet<br>yield<br>you<br>young<br>your<br>yours<br>yourself<br>youth<br>zone"""
  LISTOFCORRECTWORDS = re.split("<br>", LISTOFCORRECTWORDS) 
  LISTOFCORRECTWORDS


  def __init__(self, auto_spell_correct=True):
    self.text = None
    self.text_sens = None
    self.sens_tokens = []
    self.tokens_dict = dd(lambda : 0)
    self.ind = 0
    self.vocab_edits = None
    self.auto_spell_correct = auto_spell_correct
    pass
  
  @staticmethod
  def add_token(self, tok):
    if self.tokens_dict[tok] == 0:
      self.tokens_dict[tok] = self.ind
      self.ind += 1
    else:
      pass

  @staticmethod
  def add_sens_token(self, sen_tok):
    self.sens_tokens.append(sen_tok)
    

  def sentence_segmentation(self, text):
    text = text.lower()
    sentence_pattern = "((?<=[^\s.]{3})\s{0,}([.!?])\s{0,}(?=\w*))|\w*:"  #B.R, Dr. Br 
    text_sens = [s.strip() for s in re.sub(sentence_pattern, "<EOS>", text).split("<EOS>") if s.strip() !='']
    return text_sens
  
  def clean_words(self, word):
    punc = '[.,;:\-\(\)\]\[\$%&\*_!<>@#\\/"=]'
    return re.sub(punc,'', word)

  def tokenization(self, text_sens):
    for sen in text_sens:
      sen_tok = []
      for word in sen.split():
        cleansed_word = self.correct_words(self.clean_words(word)) if self.auto_spell_correct else self.clean_words(word)
        if not(cleansed_word in (stopwords.words('english') + [''])):
          TextProcessing.add_token(self, cleansed_word)
          sen_tok += [cleansed_word]
      TextProcessing.add_sens_token(self, sen_tok)
  
  def make_edits(self, vocab, within_edits=2):
    def one_edits(word):
      edits = set()
      for i in range(len(word)):
        stage = list(word)
        stage.pop(i)
        edits.add("".join(stage))
      return edits
    
    token_edits = {k:[] for k in vocab}
    for word in vocab:
      source = set([word])
      for epoch in range(within_edits):
        new_source = set()
        for wdrep in source:
          temp_edits = one_edits(wdrep)
          new_source = new_source.union(temp_edits)
        source = new_source
        token_edits[word].append(list(new_source))
    return token_edits

  def correct_words(self, raw_word, topn=1):
    
    top_suggestion = raw_word
    top_dis = 1e+7
    if not(raw_word in self.vocab_edits):
        raw_word_edits = self.make_edits([raw_word], within_edits=1)[raw_word][0]
  
        used_vocab = {vocab:0 for vocab in self.vocab_edits}
        suggestion_count = 0
        for rwe in raw_word_edits:
          for vocab, ves in self.vocab_edits.items():
            for dis, ve in enumerate(ves):
              if rwe in ve and not(used_vocab[vocab]) and top_dis>= dis:
                top_suggestion = vocab
                top_dis = dis
                suggestion_count += 1
                used_vocab[vocab] = 1
                break

    return top_suggestion
      

  
  def __call__(self, text):
    self.text = text

    text_sens = self.sentence_segmentation(text)   #sentence segmentation
    self.text_sens = text_sens 

    if self.vocab_edits == None and self.auto_spell_correct:
      self.vocab_edits = self.make_edits(TextProcessing.LISTOFCORRECTWORDS, within_edits=5)  #creates vocab for spell check with all edits in given range

    self.tokenization(text_sens)  #Tokenization, Punctuation Removal, Stopword Removal and Spell Check
    


#processor = TextProcessing()
#processor(text_on_topic)

"""**N-Grams**"""

from collections import Counter as c
from collections import defaultdict as d
import numpy as np


class NGrams:


  def __init__(self, train_text, preprocessing_class, N=2, **preprocessing_args):
    self.train_text = train_text
    self.preprocessor_train = preprocessing_class(**preprocessing_args)
    self.preprocessor_test = preprocessing_class(**preprocessing_args)
    self.N = N
    self.preprocessor_train(train_text)
    self.cond_probs = self.generate_probs(self.preprocessor_train.sens_tokens, n=N)
    
   
    

  def generate_n_grams(self, sens_tokens, n=2):
    n_grams = []
    for tokens in sens_tokens:
      while len(tokens) != 0:
        if len(tokens) <n :
          break
        temp = " ".join(tokens[:n])
        n_grams.append(temp)
        tokens = tokens[1:]
      
    return n_grams

  def generate_probs(self, sens_tokens, n=2):
    
    n_grams = self.generate_n_grams(sens_tokens, n)
    intotal_phrase_probs = c(n_grams) #word_probs if n==2 else c(generate_n_grams(text, n-1))
    
    cond_probs = d(lambda : {'<EOS>':1.0})
    focus_words = [phrase.split()[-1] for phrase in n_grams]
    context_words = [" ".join(phrase.split()[:-1]) for phrase in n_grams]
    context_phrase_probs = c(context_words)

    for wds in context_words:
      cond_probs[wds] = dict()

    for cwds in context_words:
      for fwds in focus_words:
        cond_probs[cwds][fwds] = intotal_phrase_probs[cwds+" "+fwds]/(context_phrase_probs[cwds])
    return cond_probs

  
  def predict_next_word(self, test_text, given_n_words=2, if_print=True):
 
    self.preprocessor_test(test_text)
    test_sens_text = [self.preprocessor_test.sens_tokens[-1]]  #most recent sentence of test text since the initiation of Ngrams class
    test_splits = self.generate_n_grams(test_sens_text, given_n_words-1)
    predicted_words = []
    for phrases in test_splits:
      top_words = sorted(list(self.cond_probs[phrases].items()), key=lambda x: x[1], reverse=True)[:3]
      predicted_words.append(top_words[0][0])
      if if_print:
        for predicted_word, score in top_words:
          print(f"\t{predicted_word}: with score {score}")
    return predicted_words


  def auto_generate(self, until_n_words=5):
    print("AI generated text : \n")
    while 1:
      start_at = input("Start Entering..:")
      if start_at == "<exit>":
        yield 1
      words = 0
      start_at = " ".join(start_at.split()[-self.N:])
      n = len(start_at.split())+1
      usgcp = False
      while words<until_n_words:
        pword = self.predict_next_word(start_at, given_n_words=n, if_print=False)
        if pword[0] == "<EOS>":
          break
        start_at = start_at.split()[1:] + pword
        print(pword[0], end=" ")
        n = len(start_at)+1
        start_at  = " ".join(start_at)
        
        words += 1
        if words >=1:
          usgcp = True
        yield 0
      print("\n")

#['played']
sorted(list(ngram_model.cond_probs['game'].items()), key=lambda x: x[1], reverse=True)[:3]
#ngram_model.predict_next_word('game', 1)
ngram_model.generate_n_grams(['game'], 1)

ngram_model = NGrams(text_on_topic, TextProcessing, N=2, auto_spell_correct=False)  #learns association of 2 words to predict 3rd
recommend = ngram_model.auto_generate()
exit=0
print("For exit enter '<exit>' when prompted")
while exit!=1:
  exit = next(recommend)

ngram_model.cond_probs['components'][

ngram_model.preprocessor.tokens_dict

#The more the database the more flexible word prompts can be, the better the algorithm understands meaning even better could be the word prompts

"""## **Basic Functionalities of WordVec**"""

"""### **vecW2V Model**"""


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Activation, Input, Dot, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from scipy.special import softmax

import copy

class vecW2V():
    


  def __init__(self, modelName):
    self.model = None
    self.probLayer = None
    self.modelName = modelName
    self.modelInputDict = None
    self.modelVecSize = None
    self.modelCorpus = None
    self.modelVocab = None
    self.modelLinedVocab = None
    self.modelWordVectors = None
    self.freqDict = None
  
  def __call__(self, originals):
    result = []
    for org in originals:
      if org in self.modelVocab:
        one_hot_vector = enc.transform(np.array([org]).reshape(-1,1))
        result.append(np.dot(one_hot_vector, self.modelWordVectors))
      else :
        result.append(np.nan)
    return result

  def removeEle(self, li, ele):
    l = li
    try:
        while True:
            l.remove(ele)
    except ValueError:
        pass
    
    return l
    
  class helperFormat():
    
    def __init__(selfi):
      return
    
    def removeEle(selfi, li, ele):
      l = li
      try:
        while True:
            l.remove(ele)
      except ValueError:
        pass
    
      return l
    
    def removeStopWords(selfi, senList):
      
      noUseWords = stopwords.words('english')
      newSenList = []
    
      for lines in senList :
        
        words = copy.deepcopy(lines.split(" "))
        for nouse in noUseWords:
          if nouse in lines.split(" "):
           
            words = selfi.removeEle(words, nouse)
      
        newSenList.append(" ".join(words))
      
      return newSenList
  
  def uniquefy(self, seq):
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not (x in seen or seen_add(x))]
  
# Removing punctuations in string
# Using loop + punctuation string
#    for ele in string: 
#      if ele in punc: 
#        string = string.replace(ele, "")
  
#    return string

  def makeCorpus(self, title=None, fromText=None, allowPunctuation=False, allowLineBreaks=True):
    
    punc = '''!()-[]{};:'"\, <>./?@#$%^&*_~'''

    if fromText == None:
      titlePage = wikipedia.page(title)
      corpus = (titlePage.content).lower()
  
    else:
      corpus = fromText.lower()

    if allowPunctuation == False:
      corpus = re.sub(r'==.*?==+', '', corpus)
      corpus = re.sub(r'[,;:\-\(\)\]\[\$%&\*_!<>@#"]','', corpus)
    
    corpus1 = re.sub(r'\n',' ', corpus)
    if allowLineBreaks == True:
      corpus2 = re.split(r'\.\s', corpus1)
    else:
      corpus2 = [corpus1]
    
    self.modelCorpus = corpus1

    return corpus1, corpus2


  def setFreqs(self):
    data = []
    freqDict = dict()
    for lis in self.modelLinedVocab.values():
      data += lis
    for vwds in self.modelVocab:
      freqDict[vwds] = data.count(vwds)
    
    self.freqDict = freqDict
    return "DONE"
    

    
  def generateSamplingDistribution(self, li, iterations=10):
    
    le = [self.modelVocab[x] for x in li]
    probDict = dict()
    totalFreqs = sum(list(self.freqDict.values()))
    for wds in le:
      probDict[wds] = (self.freqDict[wds])**0.75/(totalFreqs)**0.75
    
    pickUpList = []
    
    for wdi in li: 
      pickUpList += [wdi for x in range(ceil(probDict[self.modelVocab[wdi]]*iterations))]
    
    return pickUpList 


  def makeInputData(self, corpusSen, window, negativeSampling = 0.4, allowStopWords = False):
  
    global enc
    noUseWords = stopwords.words('english')
    togetherData = []

    vocab = []
    vocabLineWise = dict()
    lineCount = 0
    for lines in corpusSen :
      lines = re.sub("((\s+)\s)"," ", lines)
      lines = re.sub("^(\s+)|(\s+)$|","", lines)
      words = lines.split(" ")
      vocab += words
      vocabLineWise[lineCount] = words
      lineCount += 1
    
    vocab = self.uniquefy(vocab)
    if allowStopWords == False:
      for nouse in noUseWords:
        if nouse in vocab:
          vocab = self.removeEle(vocab, nouse)
    
    
    enc = OneHotEncoder(sparse=False)
    enc.fit(np.array(vocab).reshape(-1,1))

 
    for l in range(lineCount):
      temp = copy.deepcopy(vocabLineWise[l])
      for wds in vocabLineWise[l]:
      
        if wds not in vocab:
          
          temp = self.removeEle(temp, wds)
        
      vocabLineWise[l] = temp
      
    self.modelVocab = vocab
    self.modelLinedVocab = vocabLineWise
    _ = self.setFreqs()
  
    biggerOffSet = 0
    for sen in range(lineCount):
      xinputraw = vocabLineWise[sen]
    
      for c in range(len(xinputraw)):
        centreWord = xinputraw[c]

        contentIndices = range(max(0,c-window), min(len(xinputraw), c+window+1))
        negativecount = int(negativeSampling)
        candids = list(set(range(0, len(vocab))) - set(range(max(0+biggerOffSet,biggerOffSet + c-window), min(len(xinputraw)+biggerOffSet,biggerOffSet+c+window+1))))
        candids = self.generateSamplingDistribution(candids, iterations=window)

       
        if len(candids) == 0:
          noncontentIndices = [-1]
        else:
          noncontentIndices = random.choices(candids, k=negativecount)

        for pick1 in contentIndices :
          contextWord = xinputraw[pick1]
          togetherData.append((centreWord, contextWord, 1))
      
        for pick2 in noncontentIndices :
          if pick2 == -1 :
            continue
          noncontextWord = vocab[pick2]
          togetherData.append((centreWord, noncontextWord, 0))

        biggerOffSet += (len(xinputraw)-1)

    centreWordsAlone = []
    maybecontextWordsAlone = []
    similarity = []
    for dp in togetherData :
      centreWordsAlone.append(dp[0])
      maybecontextWordsAlone.append(dp[1])
      similarity.append(dp[2])
  
    xinput1 = enc.transform(np.array(centreWordsAlone).reshape(-1,1)).T
    xinput2 = enc.transform(np.array(maybecontextWordsAlone).reshape(-1,1)).T
    ylabel = np.array(similarity).reshape(1,-1)



    return xinput1, xinput2, ylabel, enc

  def makeModelAndInput(self, vecSize, corpus2, window=15, negativeSampling=5, allowStopWords=False, describeModel=True):
    
    corpusSen = corpus2
    xinput1, xinput2, ylabel, encoderObject= self.makeInputData(corpusSen, window=window, negativeSampling=negativeSampling, allowStopWords=allowStopWords)

    outputsize = ylabel.shape[0]
    xinput1 = xinput1.T
    xinput2 = xinput2.T
    ylabel = ylabel.T  #(None, 1)


    Xinp1 = Input(shape=(xinput1.shape[1],))
    Xinp2 = Input(shape=(xinput2.shape[1],))

    embed1 = Dense(vecSize, use_bias=False, name="embed1")
    embed2 = Dense(vecSize, use_bias=False, name="embed2")
    V1 = embed1(Xinp1)
    V2 = embed2(Xinp2)
    dot = Dot(axes=1)([V1, V2])
    prob = Activation('sigmoid', name="prob")(dot)
  
    model = Model(inputs=[Xinp1, Xinp2], outputs=prob, name=self.modelName)
    self.model = model
    self.probLayer = self.model.get_layer('prob')
    if describeModel == True:
      model.summary()
    
    inputDict = {"Input1":xinput1, "Input2":xinput2, "ylabel":ylabel}
    
    self.modelInputDict = inputDict
    self.modelVecSize = vecSize

    return model, inputDict

  def train(self, learning_rate=0.0009, metrics=["accuracy"], batch_size =32, epochs=100):
    model = self.model
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss="binary_crossentropy", metrics=metrics)
    model.fit(x=[self.modelInputDict["Input1"], self.modelInputDict["Input2"]], y=self.modelInputDict["ylabel"], batch_size=batch_size, epochs=epochs, verbose=2)

    self.model = model
    self.probLayer = self.model.get_layer('prob')
    _ = self.describeWordVecs()

    return model
    
  def Sort_Tuple(self, tup): 
      
    # getting length of list of tuples
    lst = len(tup) 
    for i in range(0, lst): 
        for j in range(0, lst-i-1): 
            if (tup[j][1] < tup[j + 1][1]): 
                temp = tup[j] 
                tup[j]= tup[j + 1] 
                tup[j + 1]= temp 
    return tup 

  def sig(self, x):
    z = 1/(1 + np.exp(-x))
    return z

  def describeWordVecs(self):
    
    vectorizedWords = []
    for vectors in range(self.modelInputDict["Input1"].shape[1]):
      vectorizedWords.append(self.model.weights[0][vectors,:])
    
    vectorizedWords = np.array(vectorizedWords)

    self.modelWordVectors = vectorizedWords
    return vectorizedWords

  def autoFillList(self, word1=None, word2=None, topN=None,  printList=False):
    
    vectorizedWords = self.modelWordVectors
    originWord1 = enc.transform(np.array([word1]).reshape(-1,1))
    if word2 != None:
      originWord2 = enc.transform(np.array([word2]).reshape(-1,1))
      embedWord1 = np.dot(originWord1, vectorizedWords) 
      embedWord2 = np.dot(originWord2, vectorizedWords)
      
      matches = np.dot(embedWord1, embedWord2.T)
      den = np.linalg.norm(embedWord1)*np.linalg.norm(embedWord2)
      matches = matches/den
    
      reqProbs = matches
      if printList == True:
        print(*reqProbs[0])


      return reqProbs
    else:
      embedWord = np.dot(originWord1, vectorizedWords)
  
      matches = np.dot(tf.keras.utils.normalize(embedWord), tf.keras.utils.normalize(vectorizedWords).T)
  
      reqMatches = np.flip(np.argsort(matches))[0,0:topN]

      reqProbs = np.flip(np.sort(matches))[0,0:topN]

      originWord2 = np.zeros((self.modelInputDict["Input1"].shape[1],1))
      count = 0
      result = []
      for vindex in reqMatches :
        originWord2[vindex,0] = 1
    
        result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
        if printList == True:
          print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
        count += 1
        originWord2[vindex,0] = 0


      return result

  def textCalc(self, printList=True, topN=10, **kwargs):
    vectorizedWords = self.modelWordVectors
    resultantVec = 0
    opCache = "+"
    vectorCount = 0
    if vectorizedWords.all() == None:
      return "No Vector Lookup Given"
    else :
      for keys, vals in kwargs.items():
      
        if "word" in keys:
          currentWord = enc.transform(np.array([vals]).reshape(-1,1))
          currentWord = np.dot(currentWord, vectorizedWords) 
          vectorCount +=1

          resultantVec = eval("resultantVec " + opCache + " currentWord")
        else :
          opCache = vals
    
      matches = np.dot(tf.keras.utils.normalize(resultantVec), tf.keras.utils.normalize(vectorizedWords).T)
  
      reqMatches = np.flip(np.argsort(matches))[0,0:topN]

      reqProbs = np.flip(np.sort(matches))[0,0:topN]

      originWord2 = np.zeros((self.modelInputDict["Input1"].shape[1],1))
      count = 0
      result = []
      for vindex in reqMatches :
        originWord2[vindex,0] = 1
    
        result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
        if printList == True:
          print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
        count += 1
        originWord2[vindex,0] = 0


      return result, resultantVec

  def summarizeCorpus(self, summaryTagChoices = 3, summarizeEvery = 10, printTags = True):

    vectorizedWords = self.modelWordVectors
    filteredWds = self.modelVocab
    resultantVec = 0
    taggedContent = []
    contentNum = 0
    preWd = 0
    if vectorizedWords.all() == None:
      return "No Vector Lookup Given"
    else :
      for wds in range(len(filteredWds)):
      
        if filteredWds[wds] != "*" or filteredWds[wds] != "":
          currentWord = enc.transform(np.array([filteredWds[wds]]).reshape(-1,1))
          currentWord = np.dot(currentWord, vectorizedWords) 

          resultantVec += eval("currentWord")
        
        
        else :
          continue
      
        if (wds+1)%summarizeEvery == 0:

        
          matches = np.dot(tf.keras.utils.normalize(resultantVec/summarizeEvery), tf.keras.utils.normalize(vectorizedWords).T)
          reqMatches = np.flip(np.argsort(matches))[0,0:summaryTagChoices]
          reqProbs = np.flip(np.sort(matches))[0,0:summaryTagChoices]

          originWord2 = np.zeros((self.modelInputDict["Input1"].shape[1],1))
          count = 0
          result = []

          if printTags == True:
            print("Content Number : ", contentNum, "; The Orginial Content : ", " ".join(filteredWds[preWd:wds+1]), "\n")
          for vindex in reqMatches :
            originWord2[vindex,0] = 1
    
            result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
            if printTags == True:
              print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
            count += 1
            originWord2[vindex,0] = 0
        
          if printTags == True:
            print("\n")
          contentNum += 1
          taggedContent.append(result)
          resultantVec = 0
          preWd = (wds+1)
      
      return taggedContent
  
  def predictText(self, word1=None, topN=None, printList = True):

    vectorizedWords = self.modelWordVectors
    originWord1 = enc.transform(np.array([word1]).reshape(-1,1))


    embedWord = np.dot(originWord1, vectorizedWords)

    scores =[]
    dots = np.dot(embedWord, vectorizedWords.T)
    
    for d in dots[0,:]:
      d = d.reshape(-1,1)
     
      scores.append(np.array(self.probLayer(d))[0,0])

    scores = np.array(scores)
    
    reqMatches = np.flip(np.argsort(scores))[0:topN]

    reqProbs = np.flip(np.sort(scores))[0:topN]

    originWord2 = np.zeros((self.modelInputDict["Input1"].shape[1],1))
    count = 0
    result = []
    for vindex in reqMatches :
      originWord2[vindex,0] = 1
    
      result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
      if printList == True:
        print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
      count += 1
      originWord2[vindex,0] = 0


    return result



  def visualiseWordVec(self, targetDimensions=2, vecCount = None, model = None):

    filteredWds = self.modelVocab
    vectorizedWords = self.modelWordVectors
    labels = []
    tokens = []

    if model == None:

      for wds in range(len(filteredWds)):
      
          if (filteredWds[wds] != "*" or filteredWds[wds] != "" )and (filteredWds[wds] not in labels):
            currentWord = enc.transform(np.array([filteredWds[wds]]).reshape(-1,1))
            labels.append(filteredWds[wds])
            tokens.append(np.dot(currentWord, vectorizedWords)[0]) 
    else:
      
      for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)

    
    #print(tokens[0:2])
    Vtsned = TSNE(n_components=targetDimensions).fit_transform(tokens)

    x = []
    y = []
    for value in Vtsned:
        x.append(value[0])
        y.append(value[1])
        
    plt.figure(figsize=(16, 16))
    if vecCount == None:
      num = len(x)
    else:
      num = vecCount 
    for i in range(num):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

    return

"""### **vecW2V on Wikipedia**"""

corpus = """Viruses are microscopic organisms that exist almost everywhere on earth. They can infect animals, plants, fungi, and even bacteria.

Sometimes a virus can cause a disease so deadly that it is fatal. Other viral infections trigger no noticeable reaction.

A virus may also have one effect on one type of organism, but a different effect on another. This explains how a virus that affects a cat may not affect a dog.

Viruses vary in complexity. They consist of genetic material, RNA or DNA, surrounded by a coat of protein, lipid (fat), or glycoprotein. Viruses cannot replicate without a host, so they are classified as parasitic.

They are considered the most abundant biological entity on the planet.Almost every ecosystem on Earth contains viruses.

Before entering a cell, viruses existTrusted Source in a form known as virions.

During this phase, they are roughly one-hundredth the size of a bacterium and consist of two or three distinct parts:

genetic material, either DNA or RNA
a protein coat, or capsid, which protects the genetic information
a lipid envelope is sometimes present around the protein coat when the virus is outside of the cell
Viruses do not contain a ribosome, so they cannot make proteins. This makes them totally dependent on their host. They are the only type of microorganism that cannot reproduce without a host cell.

After contacting a host cell, a virus will insert genetic material into the host and take over that hostâ€™s functions.

After infecting the cell, the virus continues to reproduce, but it produces more viral protein and genetic material instead of the usual cellular products.

It is this process that earns viruses the classification of parasite.

Viruses have different shapes and sizes, and they can be categorized by their shapes.

These may be:

Helical: The tobacco mosaic virus has a helix shape.
Icosahedral, near-spherical viruses: Most animal viruses are like this.
Envelope: Some viruses cover themselves with a modified section of cell membrane, creating a protective lipid envelope. These include the influenza virus and HIV.A virus exists only to reproduce. When it reproduces, its offspring spread to new cells and new hosts.

The makeup of a virus affects its ability to spread.

Viruses may transmit from person to person, and from mother to child during pregnancy or delivery.

They can spread through:

touch
exchanges of saliva, coughing, or sneezing
sexual contact
contaminated food or water
insects that carry them from one person to another
Some viruses can live on an object for some time, so if a person touches an item with the virus on their hands, the next person can pick up that virus by touching the same object. The object is known as a fomite.

As the virus replicates in the body, it starts to affect the host. After a period known as the incubation period, symptoms may start to show.

What happens if viruses change?
When a virus spreads, it can pick up some of its hostâ€™s DNA and take it to another cell or organism.

If the virus enters the hostâ€™s DNA, it can affect the wider genome by moving around a chromosome or to a new chromosome.

This can have long-term effects on a person. In humans, it may explain the development of hemophilia and muscular dystrophy.

This interaction with host DNA can also cause viruses to change.

Some viruses only affect one type of being, say, birds. If a virus that normally affects birds does by chance enter a human, and if it picks up some human DNA, this can produce a new type of virus that may be more likely to affect humans in future.

This is why scientists are concerned about rare viruses that spread from animals to people."""
#corpus = ".".join(corpus.split("\n"))

from scipy import stats
vecM = vecW2V("MineVectorizer")

_, corpus = vecM.makeCorpus(title="guitar")


print("MineVectorizer Model Description : \n")
modelalt, inputDict = vecM.makeModelAndInput(30, corpus, window=5, negativeSampling=4, allowStopWords=True)


print("MineVectorizer Model Training : \n")
modelalt = vecM.train(epochs=50, learning_rate=0.001)

print("MineVectorizer Suggestions : \n")
choices = vecM.autoFillList(word1="red", topN=20, printList=True)

print("MineVectorizer Visualisations : \n")
vecM.visualiseWordVec(targetDimensions=2, vecCount=5)


print("MineVectorizer Description : \n")
print(vecM.model, vecM.modelCorpus, vecM.modelVocab, vecM.modelInputDict)
#print(corpus)"""

#Additional Training if required
_ = vecM.train(learning_rate = 0.001, epochs=10000)

vecM.modelWordVectors

vecM.modelCorpus

_ = vecM.autoFillList(word1="especially", topN=20, printList=True)

_ = vecM.summarizeCorpus(summaryTagChoices=3, summarizeEvery=2030)

_ = vecM.textCalc(word1="sound", op1="+", word2="vibrates", op2="+", word3="instrument")

vecM.visualiseWordVec(vecCount=1000)

#corpus = """The Xbox Series X and the Xbox Series S (collectively the Xbox Series X/S[b]) are home video game consoles developed by Microsoft They were both released on November 10, 2020 as the fourth generation of the Xbox console family succeeding the Xbox One family Along with Sony's PlayStation 5 also released in November 2020, the Xbox Series X and Series S are part of the ninth generation of video game consoles. Rumors regarding the consoles first emerged in early 2019, with the line as a whole codenamed "Scarlett", and consisting of high-end and lower-end models codenamed "Anaconda" and "Lockhart" respectively. Internally, Microsoft had been satisfied with the two-console approach for the Xbox One, and planned a similar approach for the fourth generation Xbox, with the target for the high-end model to at least double the performance of the Xbox One X. The high-end model was first teased during E3 2019 under the title "Project Scarlett", while its name and design as Xbox Series X was unveiled during The Game Awards later in December. In September 2020, Microsoft unveiled the lower-end model as the Xbox Series S. The Xbox Series X has higher end hardware, and supports higher display resolutions (up to 8K resolution) along with higher frame rates and real-time ray tracing; it also has a high-speed solid-state drive to reduce loading times. The less expensive Xbox Series S uses the same CPU, but has a less powerful GPU, has less memory and internal storage, and lacks an optical drive. Both consoles are backwards compatible with many previous generation Xbox games, controllers, and accessories. As part of a program Microsoft calls "Smart Delivery", many previous generation games feature upgraded graphics on the Series X/S at no additional charge. The consoles are also compatible with the gaming subscription service Xbox Game Pass, as well as the cloud game-streaming platform Xbox Game Pass cloud gaming."""

#corpus = """Economic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Economic precepts occur throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod himself as the "first economist". Other notable writers from Antiquity through to the Renaissance include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as "coming nearer than any other group to being the "founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.[34][failed verification] A seaport with a ship arriving A 1638 painting of a French seaport during the heyday of mercantilism Two groups, who later were called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies. Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy. Adam Smith (1723â€“1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject."""
vecM2 = vecK("MineVectorizer_2")

corpus = vecM2.makeCorpus(title="economics")

#print("MineVectorizer Model Description : \n")
vecM2.makeModelAndInput(20, corpus)

#print("MineVectorizer Model Training : \n")
vecM2.train(learning_rate=0.06, epochs=30)

#print("MineVectorizer Suggestions : \n")
vecM2.autoFillList(word1="greek", topN=20, printList=True)

#print("MineVectorizer Visualisations : \n")
vecM2.visualiseWordVec(targetDimensions=2, vecCount=100)

#print("MineVectorizer Description : \n")
#print(vecM.model, vecM.modelName, vecM.modelWordVectors, vecM.modelVocab, vecM.modelVecSize, vecM.modelInputDict)
#print(corpus)

vecM2.visualiseWordVec(vecCount=400)

_ = vecM2.autoFillList(word1="money", word2="monetary", topN=10, printList=True)
#vecM2.modelCorpus

#The doctrine called for importing
#operatedWord = textCalc(vectorizedWords=vectorizedWords, word1="doctrine", op1="+", word2="called", op2="-", word3="importing")
summarizedLines = vecM2.summarizeCorpus(summaryTagChoices= 10, summarizeEvery= 100)
#print(filteredWds)

_, resultantVec1 = vecM2.textCalc(topN=0, word1="money", op1="+", word2="system")
_, resultantVec2 = vecM2.textCalc(topN=0, word1="monetary")
#_, resultantVec3 = vecM2.textCalc(topN=0, word1="system")
print(tf.keras.utils.normalize(resultantVec1))
print(resultantVec2)
#print(resultantVec3)
print(model1.wv.most_similar(positive=["money", "system"], topn=0), model1.wv.most_similar(positive=["monetary"], topn=0))

"""### **vecW2V on IMDB**"""

(xtrain, ytrain), (xtest, ytest) = imdb.load_data()

xtrain

"""### **genism On Wikipedia**"""

pip install gensim

corpus = """Economic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Economic precepts occur throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod himself as the "first economist". Other notable writers from Antiquity through to the Renaissance include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as "coming nearer than any other group to being the "founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.[34][failed verification] A seaport with a ship arriving A 1638 painting of a French seaport during the heyday of mercantilism Two groups, who later were called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies. Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy. Adam Smith (1723â€“1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject."""

import gensim
from gensim.models import Word2Vec
data = []
testModel = vecK("testModel")
helperModel = testModel.helperFormat()
corpusSen = helperModel.removeStopWords(testModel.makeCorpus(title="guitar")[1])
for sen in corpusSen :
  data.append(sen.lower().split(" "))

model1 = gensim.models.Word2Vec(data, min_count = 1, 
                              size=40, window = 25, sg=1, negative=5)#0.81516844

# Commented out IPython magic to ensure Python compatibility.
import gensim
from gensim.models import Word2Vec
import numpy as np

# Get the interactive Tools for Matplotlib
# %matplotlib notebook
import matplotlib.pyplot as plt
plt.style.use('ggplot')

from sklearn.decomposition import PCA

from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
data = [["red", "fox"], ["blue", "fox"], ["orange", "fox"], ["red", "dog"], ["blue", "dog"], ["orange", "dog"], ["fox", "jumped"],["dog", "jumped"]]

model1 = gensim.models.Word2Vec(sentences=data, min_count=1, sg=1, size=2)#0.81516844
model1.wv.most_similar("red")

model1.wv.most_similar("fox")
def display_pca_scatterplot(model, words=None, sample=0):
    if words == None:
        if sample > 0:
            words = np.random.choice(list(model.vocab.keys()), sample)
        else:
            words = [ word for word in model.vocab ]
        
    word_vectors = np.array([model[w] for w in words])

    twodim = PCA().fit_transform(word_vectors)[:,:2]
    
    plt.figure(figsize=(6,6))
    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')
    #for word, (x,y) in zip(words, twodim):
    #    plt.text(x+0.05, y+0.05, word)

display_pca_scatterplot(model1, ["red", "fox", "blue", "dog"])

display_pca_scatterplot(model1, words=["red", "fox", "blue", "dog"])

corpusSen

model1.wv.most_similar("steel", topn=20)
#model1.wv.similarity("domestic", "cat")

result = model1.wv.most_similar(positive=['econometrics,'], negative=['mathematical,'], topn=1)
print(result)

vecM2.visualiseWordVec(model=model1, vecCount=400)
#print(vectorizedWords)

"""## **Chat Predict**

### **Corpus** ($Not A Good Corpus To Test On$)

* Pranu: Done jay mama I started my app installations with whatâ€™s app  Jay Mama: Hi Pranu  Pranu: Hi jay mama  Pranu: Will update my profile pic soon  Jay Mama: How to add u to Kala group  Jay Mama: Hey nice DP  Pranu: Thank you jay mama  Jay Mama: Down load Pinterest and Myntra app  Jay Mama: Start shopping  Pranu: Ok  Jay Mama: If want to buy any games buy it donâ€™t hesitate  Pranu: Thank u ,thank u ,thank u lovely jay mama  Pranu: I will use it when required  Pranu: Jay mama I got an alert that some one is trying to sign into my iCloud account.Is it something to worry??  Jay Mama: Send me that mail  Pranu: Not the mail it was a flash messenger  Pranu: *message  Jay Mama: Oh ok did u get any one time password  Pranu: No I got something like this â€œDo you want to allow access to so and so personâ€  Pranu: And I opted no donâ€™t allow  Jay Mama: Ok  Jay Mama: Look into this   Pranu: Thank u jay mama I will definitely look into this.  Jay Mama: Very useful stuff for you Pranu. Kindly read it when ur free   Pranu: This is the same message I got and when checked the YouTube I found that the message is false so can I just relax now? Or is there anything still to worry about??  Jay Mama: Donâ€™t install anything  Jay Mama: Call me  Pranu: Jay mama could u send me the account details of Netflix. I am not able to stream it on my tv due to poor internet connectivity with my router  Pranu: So I need account details to install the app and login into my shared Netflix account .  Jay Mama: Netflix account details :  Pranu: Thanks jay mama  Pranu: Pops up a message â€˜incorrect passwordâ€™  Jay Mama: Try again properly Pranu.  Pranu: Copy pasted it jay mama  Pranu: Same result  Jay Mama: Ok wait  Jay Mama: Changed Netflix password - jharicv@me.com, Password- JAYAcv5670!  Pranu: Worked!!! Thanks jay mama....  Pranu: Nanna cell ki phone chae mama  Jay Mama: Ok  Jay Mama: Give ur gmail is  Pranu: Thanks jay mama. I have seen it. Itâ€™s helpful  Jay Mama: Pranu sleep ra enough ra  Jay Mama: Thanks a lot nana. Will call u tomorrow  Pranu: No prob jay mama itâ€™s holidays  Jay Mama: Donâ€™t post anymore pics. People will be tried  Jay Mama: Just for fun nana  Pranu: Chill!!!! Jay mama  Pranu: Need not specify jay mama I aspire this humour attitude of yours  Jay Mama: Pranu do u have bank account  Pranu: Nooope  Pranu: Why?!  Jay Mama: After lockdown open. Open one bank pranu.  Jay Mama: I shall tell later  Pranu: Ok Jay mama  Pranu: Done ðŸ‘  Pranu: This above one is an intro editing done by me today for our short film  Pranu: This one has more editing elements in it and the banner that appears at the first is self created  Pranu: Jay mama could you call me now?  Pranu: Jay mama we though of giving vehicles, other than those that belong to Hyundai, a try as we wanted our car to be something new into the society like Honda City  or Kia Seltos or MG  Jay Mama: Ok nice  Jay Mama: Whoâ€™s is driving  Jay Mama: The car  Jay Mama: You or dad  Pranu: Well both  Pranu: Just like the i20  Pranu: Thoughts about my car havenâ€™t been pondered over yet  Pranu: Well even Venue is new into the society but yet weâ€™ll didnâ€™t like the model for unknown reasons  Jay Mama: Ya check on those website and videos in our budget  Pranu: Hmm.. currently on it in fact weâ€™re planning to pay a visit to the Honda City and Kia showrooms  Pranu: To have a clear set look at the cars  Jay Mama: Ok ðŸ‘ðŸ»  Pranu: Ok jay mama I think itâ€™s real mid night for u good night then bye  Jay Mama: Yes nana to tomorrow  Jay Mama: I have flights  Jay Mama: To Virginia  Pranu: Ohh  Pranu: Jay mama, yesterday after u said abt those anonymous logins into Babayeâ€™s account, I just intimated all my friends in a soft tone to double check that they are entering Netflix only through my profile, but they replied they never did enter any other profile and also i personally keep checks on them and their viewing history regularly. For sure these are not the guys and I guess it could have been Pranuâ€™s  friends. Just guess  Jay Mama: Ok no problem  Jay Mama: Itâ€™s all Part of youth life  Pranu: I know but just making it clear that itâ€™s not my friends and by the way, besides cautioning my friends about login I myself ironically logged into Babayeâ€™s account by mistake this morning! I know itâ€™s coming youth life as u said but this time itâ€™s not we  That was a punch line ðŸ˜ðŸ˜  Pranu: Okay GN jay mama, might have been really tiered  Pranu: jay mama, Found this on youtube  Pranu: though in this video it shows how to connect PC to IMac, it also works with a windows laptop  Pranu: Perks: No need of any ports or cables everything\'s done wirelessly  Pranu: Loop hole: I need to uninstall windows seven residing on my IMac and install windows 10 for all of this to happen I need to get my mac keyboard fixed in the first place  Jay Mama: Check the cable option pranu  Pranu: Iâ€™ll but why cable when it can be done without cable?  Pranu: We need a HDMI to Thunderbolt connector  Jay Mama: Pranu login to amazon venkatajay567@gmail.com, JAYAcv5670!  Jay Mama: To watch man who knew infinity  Pranu: Oh great!! thanks a jay mama  Jay Mama: Watch before 48 hours from now nana  Pranu: Sure jay mama!! Morning I have classes to take so Iâ€™ll watch it tonight  Jay Mama: .com  Pranu: Ok  Pranu: Otp pls  Jay Mama: 913035 is your Amazon OTP. Do not share it with anyone.  Pranu: Done jay mama  Jay Mama: Enjoy ðŸ˜Š  Pranu: Just now finished watching the movie. Thatâ€™s been a great movie to watch. My respect for him grew at least by a bit more after watching this film. All thanks to you Jay mama!!! This is one is a cracker of a Biopic for sure  Jay Mama: True ðŸ‘ðŸ»  Jay Mama: Pranu what is the app to create to video with music apart from iMovies  Pranu: Use Filmora jay mama  Pranu: Licensed e-mail: c2941647@drdrb.com Registration code: 10403029CF3644154841651AF141E800 Licensed e-mail: c2941690@drdrb.com Registration code: 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Registration code: 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2942269@drdrb.com Registration code: 00289623F7B3B81E14AEB526144B6D08 Licensed e-mail: bidjan@ziggo.nl  Key : CE8B0909EEC77B27DFEA94190F3A0223 Licensed e-mail: c2941690@drdrb.com Key: 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Key: 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2943267@drdrb.com Key: D772BE0279AFE60AF0E1D2109CA89A19 Licensed e-mail: c2943297@drdrb.com Key: FB9694298253B51545E70D22B3033808 Licensed e-mail: c2941647@drdrb.com Key: 10403029CF3644154841651AF141E800 Licensed e-mail: c2941690@drdrb.com  Key : 510B3C20A9E54E0FF1D2FC28BAD1220E Licensed e-mail: c2942163@drdrb.com Key : 35706E040C63EE00E377911BB9A3B301 Licensed e-mail: c2942269@drdrb.com  Key : 00289623F7B3B81E14AEB526144B6D08 Licensed e-mail: c2943267@drdrb.com Key : D772BE0279AFE60AF0E1D2109CA89A19 Licensed e-mail: c2943297@drdrb.com Key : FB9694298253B51545E70D22B303380  Pranu: And jay mama use the above lisence mail IDs  Pranu: To remove water mark  Pranu: From the videos created using filmora with out having to subscribe for the paid version  Jay Mama: Nice DP pranu  Pranu: Thank u jay mama. so the DP is nice to be noticed....  Pranu: Hmm  Pranu: Ignore all those deleted messages Jay mama, they were meant to be sent to my friends just some pics of my code work  Jay Mama: Ok thanks pranu  Pranu: Raw!!! Yet funny. In fact Jay mama in our batch too there was a guy, he attended the college for the first 2-3 months of first year and disappeared suddenly and finally just before 2 months to IIT exams in second year he appeared. He was the centre of attraction then when sirs asked him what happened to him all these 1:5 years, he said like he was preparing for CA exams and now he wanted to prepare for JEE so he came  Pranu: Almost similar to answers given in the video but no cuss words  Pranu: Finally, lecturers took him in as he has a point â€œI paid fee now whatâ€™s ur problemâ€  Pranu: Though it doesnâ€™t work always, yet itâ€™s still a fun always to fool others. Isnâ€™t it??!!  Pranu: Jay mama where do u get your WhatsApp stickers from?  Jay Mama: Form whom ?  Jay Mama: From whom ?  Pranu: Like which app?  Pranu: There are very limited out there for iPhones right?  Jay Mama: Meme ?  Pranu: Yeah  Pranu: Like WhatsApp stickers  Jay Mama: Tenor  Pranu: Yeah this one  Pranu: Great thanks jay mama  Pranu: Latest news right Jay mama? This seems to be lot cooler than Xbox Series X. What do u say?  Pranu: Hmm everythingâ€™s great but then what abt Optical drive thingy Jay mama? We donâ€™t  have it in Xbox S right?  Pranu: Like what are the alternatives to the games if not CDS?  Pranu: Jay mama did u receive any mail regarding verification code from Tatasky  Pranu: If so could u send the code  Pranu: Asap my friend is accessing our Tatasky to watch IPL  Jay Mama: To which email Iâ€™d  Pranu: The one starting with J  Pranu: Any such?  Jay Mama: No pranu I didnâ€™t receive  Jay Mama: Why my email is used ?  Pranu: Ok jay mama donâ€™t know  Pranu: Like hold on  Pranu: This is what it showed  Pranu: The mail there started with j  Pranu: And Nanna is a out with low internet thatâ€™s why heâ€™s not receiving the OTPs  Jay Mama: Ok wait let me look deeply in Gmail  Pranu: Hmm only if u r free Jay mama  Jay Mama: Cancel and request again  Pranu: Ok on it  Pranu: Jay mama relax seems like my friend accessed wrong website he went to the accounts website  Pranu: Might get solved without ur intervention  Jay Mama: Oh no ðŸ¤¦\u200dâ™‚ï¸  Jay Mama: Ok let me know if any nice DP  Pranu: Thank u Jay mama  Pranu: Hmm Iâ€™ll make note of this Jay mama  Jay Mama: I think it would be expired  Pranu: Why? So should o reset the details again  Pranu: ?  Pranu: *I  Jay Mama: Request again  Jay Mama: I shall share the password  Pranu: Ok  Pranu: Jay mama u should have got it  Jay Mama: Password- ybg6T5  Pranu: Done jay mama  Pranu: Thanks  Pranu: I was waiting for the reset password to come to Nannaâ€™s phone but it didnâ€™t show up  Jay Mama: Cool  Pranu: And so were those multiple requests  Pranu: ðŸ˜‚ðŸ¤£ðŸ˜‡  Pranu: *last emoji typo  Pranu: ðŸ˜‚  Pranu: Super funny jay mama perfect lip sink  Pranu: The "Le Petit Chef " restaurant in France, came up with an original way to entertain guests while waiting for their order by using an overhead 3D projector on the ceiling.\xa0 The animation is on the table and your plate. There is a small chef who appears on your plate, and thatâ€™s only the beginning.  Pranu: Jay mama we feel the chair is more of a issue here than the table. So also send ur recommendations on chairs  Pranu: The chair u have now would do  Jay Mama: That chair is not available in ikea India Pranu  Pranu: Can we find the same model in any other stores  Pranu: Or any model similar to that  Pranu: I too will search now once  Pranu: Could u send me ur chair model so that I can search  Jay Mama: Really Epic ðŸ˜ƒ  Pranu: ðŸ˜‚ðŸ¤£  Pranu: Valid point to argue with  Pranu: ðŸ˜‚  Pranu: Jay mama Iâ€™ll send u the credentials In abt 20 mins or so having dinner  Jay Mama: Ok  Pranu: Mail ID : prasad5670@gmail.com Password : Hometheatre6  Pranu: Jay mama  Pranu: My keyboard thing we went through all the stores Apple non Apple but no use it needs to be replaced  Pranu: I have searched for the prices and these are the estimates : Non numerical keyboard  8900 Numerical Keyboard   Non Numerical Keyboard  Numerical keyboard  I prefer Numerical cause itâ€™s the latest one and also chargeable no head ache of batteries being corroded or stuck again  Pranu: So which way is it? Shall I buy it here or will u buy there?  Pranu: Jay mama placed order for a 300 rs USB keyboard  Pranu: And also checked out and found that we can connect through USB keyboards at login  Pranu: So this should work  Jay Mama: Super  Pranu: And also Guest user account is locked from inside so thatâ€™s out of thought now  Pranu: Jay mama got the USB keyboard and itâ€™s working  Pranu: So Jay mama suggest a nice Bluetooth keyboard  Jay Mama: Will look into  Jay Mama: Super  Pranu: ðŸ‘  Pranu: Jay mama  Pranu: Have a look at this  Pranu: I pretty much like it  Jay Mama: Ya good will that keyboard work for  coding ?  Pranu: Yep  Jay Mama: How much  Pranu: 52$  Pranu: 3744 rs  Pranu: Searching for Indian price  Pranu: And stock  Pranu: Ya like price tag mentioned in dollars but deliver to India option available  Jay Mama: Ok let me check  Pranu: Jay mama Indian price 11000  Pranu: I think I should find some other model in Logitech itself I pretty much liked their button design  Pranu: Check out "Oke Oka Jeevitham" from Mr. Nookayya on JioSaavn!  Pranu: Jay mama we are planning to replace our old TV with a new one!! Do u know of any new models? If free call  Jay Mama: Oh nice  Jay Mama: will call you nana  Pranu: Ya ok jay mama no hurry  Pranu: Jay mama my old profile in Netflix is gone however I have an option to add profile!! Iâ€™m creating a new one under your account  Pranu: Jay mama waiting for your feed  Pranu: Jay mama how long could it take?  Jay Mama: We are ready  Pranu: ðŸ¤£ðŸ¤£  Jay Mama: Call me pranu  Jay Mama: When your free  Pranu: This is a lengthy review and neither me has gone through it but seems like a good one from the views and the guy whoâ€™s giving it! Anyways, Iâ€™m hell bent on to XBOX cause I fell for the Game â€œForza horizon 4â€ anyways have a look Jay mama! And state your opinion!!  Jay Mama: Oh super will check our pranu  Jay Mama: Let me check other reviews  Jay Mama: As well  Pranu: Ya sure Jay mama!  Jay Mama: Donâ€™t post that mass video ra  Jay Mama: Itâ€™s funny ðŸ˜„  Pranu: This review was made much before the release of either of these consoles but it was pretty much right about Hardware specs! So might help Jay mama!!  Pranu: Jay mama did u review about buying the XBOX?  Pranu: Iâ€™m just a so very curious to have it asap  Jay Mama: All sold out pranu  Jay Mama: Waiting for to be back in stock  Jay Mama: Is it available in India ?  Pranu: Ya Jay mama like on Amazon  Jay Mama: Really  Jay Mama: How much ?  Jay Mama: Is it officially launched in India  Pranu: Oh wait  Pranu: Out it stick Jay mama  Pranu: But ya itâ€™s officially released and costs  Pranu: 49,900  Pranu: *out of stock  Jay Mama: In USA 37,000  Jay Mama: 12k difference  Jay Mama: But stock is not there  Jay Mama: Our today discussion in video. Watch it when free pranu  Pranu: That was a great talk Jay mama. This is really really gone shape the way I think whenever I think of innovations.  Pranu: Iâ€™ll keep following his talks now on  Pranu: This is our elective course offering Jay mama itâ€™s not available to us now but anyway would be available in third year  Pranu: These are all the minors that are being offered this Sem  Jay Mama: Sure Pranu will checkout and let you know by Sunday  Pranu: Ya jay mama no probs  Pranu: But just be known that the deadline for this registration is 31.1.2021 and itâ€™s limited seats  Jay Mama: This is really interesting course pranu. Take it whenever available  Jay Mama: AI & ML course feels valuable. What u think nana ?  Pranu: Ya but this is for other departments other than CSE cause we will get this in the main stream in third year  Pranu: So this would be available in third year Jay mama as it is an elective so Iâ€™ll pick it when offered to me  Pranu: So Jay mama I feel for this sem I could just start an online psychology course and let go the minors. And in the third sem as you said I could pick the IDEA 104 course as elective  Jay Mama: Ok ðŸ‘ðŸ»  Pranu: Because taking any minor now could burden the work of taking that interesting IDEA 104 course in next year  Pranu: Because minor lasts for 4 years  Jay Mama: Oh nice , Do whatever u feel interesting u know better than anyone  Pranu: True ok then this sem a self learning online Yale university psychology course and next year the BBA elective  Pranu: Watch this on Amazon Prime Jay mama!! Really a great One!  Jay Mama: Ok will do  Jay Mama: I would say to wait pranu.  Jay Mama: We donâ€™t if it real of products have been replaced or not ?  Pranu: Ya sure Jay mama  Pranu: And also I got to know thatâ€™s Xbox series X  Pranu: Is getting available  Pranu: On Amazon  Pranu: Soon  Pranu: Like from April  Jay Mama: U shall get in a month or 2.  Jay Mama: Iâ€™m planning to India in June  Pranu: Ya thatâ€™s what! Just to make an enquiry of reliability of this website I posted the image  Pranu: Great News!  Pranu: Jay mama just now while chatting with my friends, it just came up that you pay for my Netflix account! And they were like â€œur uncle is super cool!! Netflix is costly and itâ€™s super cool that you are enjoying Netflix for free because of your uncleâ€  Pranu: Thatâ€™s really cool Jay mama! ðŸ˜ŽðŸ˜ŽItâ€™s not just me who enjoyed but also shared it to my other friends for free, all thanks to you!  Pranu: ðŸ¤£ðŸ¤£  Pranu: #GifKing  Jay Mama: X box series x Kada pranu  Pranu: Ejactly  Pranu: Jay mama  Pranu: Hmm â€œout of stockâ€!! in a very great demand  Jay Mama: They are not making enough products. Donâ€™t know why  Pranu: I gotta a news that soon itâ€™s going to be available on Amazon  Pranu: Something like from April  Jay Mama: Letâ€™s wait and see PlayStation is available  Pranu: It seems that people with computers and tech knowledge are using Bots to buy Xbox soon after they get into stores!! Since bots are faster than humans they are making transactions faster and these tech guys are later selling the products for double the price  Pranu: Ya but Jay mama thereâ€™s actually nothing good to play on play station like I would like to play Forza, red dead redemption and games like that they are top notch  Pranu: And Xbox series S is that we have to pay subscription so thatâ€™s gonna cost us more than series X  Pranu: In two to three years  Pranu: Which could be played on Xbox alone By the way  Jay Mama: Oh no ðŸ¤¦\u200dâ™‚ï¸ Inka wait cheyyaddam. Thappa we canâ€™t do anything Kada  Pranu: Yaa no probs seems like series x gonna come soon  Pranu: I waited this long Iâ€™m ready to wait a couple more months  Pranu: Because even if I get Series S or play station I wonâ€™t be satisfied  Pranu: Available on Amazon Prime  Jay Mama: Nice review   Pranu: Ya really a nice one  Jay Mama: How US college admissions works ? Seen â€œOperation Varsity Blues:\xa0The\xa0College\xa0Admissions\xa0Scandalâ€ on Netflix yet?  Pranu: Watched the trailer Jay mama seems pretty much interesting will give it a watch  Pranu: For sure  Jay Mama: So funny pranu  Jay Mama: Similar to somebody , nobody jokes  Pranu: Ya ðŸ˜†ðŸ˜†  Pranu: Whoâ€™s not gonna like it?  Pranu: Jay mama suggest me a power bank for the phone I badly need one!  Pranu: Especially with a lower battery capacity for the iPhone 7  Jay Mama: What happened any problem with phone pranu  Pranu: Generally the battery of iPhone 7 is less right it gets completed soon and also recently while taking exam the power was gone and I had 7% charge on the phone so I felt the need of a power bank and also that would be helpful when I go to college Jay mama  Pranu: So even amma was asking for a power bank without need to carry a charger   Jay Mama: Ok buy 10000 or 15000 mah power bank  Pranu: Ya thought the same Jay mama! So MI power bank would be nice right?  Jay Mama: Yes ðŸ‘ðŸ»  Jay Mama: All are same  Pranu: Ohh ok  Jay Mama: Eey killer DP pic ðŸ‘ðŸ»ðŸ‘ðŸ»ðŸ‘ðŸ»  Pranu: Thank you Jay mama*

### **My Predictions**
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Activation, Input, Dot, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from scipy.special import softmax

#======================================KEY TO WORD2VEC REMOVE NOISE FROM DATA AND WAIT FOR THE NN TO REA
#corpus = """The Xbox Series X and the Xbox Series S (collectively the Xbox Series X/S[b]) are home video game consoles developed by Microsoft They were both released on November 10, 2020 as the fourth generation of the Xbox console family succeeding the Xbox One family Along with Sony's PlayStation 5 also released in November 2020, the Xbox Series X and Series S are part of the ninth generation of video game consoles. Rumors regarding the consoles first emerged in early 2019, with the line as a whole codenamed "Scarlett", and consisting of high-end and lower-end models codenamed "Anaconda" and "Lockhart" respectively. Internally, Microsoft had been satisfied with the two-console approach for the Xbox One, and planned a similar approach for the fourth generation Xbox, with the target for the high-end model to at least double the performance of the Xbox One X. The high-end model was first teased during E3 2019 under the title "Project Scarlett", while its name and design as Xbox Series X was unveiled during The Game Awards later in December. In September 2020, Microsoft unveiled the lower-end model as the Xbox Series S. The Xbox Series X has higher end hardware, and supports higher display resolutions (up to 8K resolution) along with higher frame rates and real-time ray tracing; it also has a high-speed solid-state drive to reduce loading times. The less expensive Xbox Series S uses the same CPU, but has a less powerful GPU, has less memory and internal storage, and lacks an optical drive. Both consoles are backwards compatible with many previous generation Xbox games, controllers, and accessories. As part of a program Microsoft calls "Smart Delivery", many previous generation games feature upgraded graphics on the Series X/S at no additional charge. The consoles are also compatible with the gaming subscription service Xbox Game Pass, as well as the cloud game-streaming platform Xbox Game Pass cloud gaming."""

#corpus = """Economic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Economic precepts occur throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod himself as the "first economist". Other notable writers from Antiquity through to the Renaissance include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as "coming nearer than any other group to being the "founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.[34][failed verification] A seaport with a ship arriving A 1638 painting of a French seaport during the heyday of mercantilism Two groups, who later were called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies. Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy. Adam Smith (1723â€“1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject."""
chatPredict = vecK("MineVectorizer_3")

_,corpusSen = chatPredict.makeCorpus(fromText=corpus)

#print("MineVectorizer Model Description : \n")
chatPredict.makeModelAndInput(30, corpusSen, window=25)

#print("MineVectorizer Model Training : \n")
chatPredict.train(epochs=10, learning_rate=0.005)

#print("MineVectorizer Suggestions : \n")
chatPredict.autoFillList(word1="netflix", topN=20, printList=True)

#print("MineVectorizer ChatPredict : \n")
_ = chatPredict.predictText(word1="netflix", topN=10, printList=True)

#print("MineVectorizer Visualisations : \n")
#chatPredict.visualiseWordVec(targetDimensions=2, vecCount=300)

#print("MineVectorizer Description : \n")
#print(vecM.model, vecM.modelName, vecM.modelWordVectors, vecM.modelVocab, vecM.modelVecSize, vecM.modelInputDict)
#print(corpus)

chat = open('/content/jaymama_chat.txt')
data = chat.read()
data = data.splitlines()
chat.close()

clean_data = []
pattern = "\[([^\[\]]+|(?0))*]"
for chat in data :
  rechat = re.sub("[\(\[].*?[\)\]]", "", chat) 
  if "\u200e" in rechat or "Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them." in rechat or len(rechat.split(" "))<=1 or "https" in rechat:
    continue
  clean_data.append(rechat)

corpusSen = clean_data
corpus = " ".join(clean_data)
#corpusSen = wikiCorpusSen

_ = chatPredict.train(epochs=20, learning_rate=0.05)

_ = chatPredict.autoFillList(word1="prime", topN=5, printList=True)

_ = chatPredict.predictText(word1="yale", topN=5)

_ = chatPredict.autoFillList(word1="52", topN=10, printList=True)

chatPredict.visualiseWordVec(vecCount=100)

_ = chatPredict.summarizeCorpus(summaryTagChoices=5, summarizeEvery=750)

_,vectorn1 = chatPredict.textCalc(word1="xbox", printList=False)
_,vectorn2 = chatPredict.textCalc(word1="playstation", printList=False)
print(vectorn1, vectorn2)

"""### **Original Predictions**"""

pip install gensim

import gensim
from gensim.models import Word2Vec
data = []
for sen in corpusSen :
  data.append(sen.lower().split(" "))

model1 = gensim.models.Word2Vec(data, min_count = 1, 
                              size = 30, window = 25, sg=1, negative=10)#0.81516844

model1.wv.most_similar("playstation", topn = 20)
#model1.wv.similarity("netflix", "account")
#print(model1['jay'], model1['mama'])

"""## **Predict the Context Words**"""

corpus = """The Xbox Series X and the Xbox Series S (collectively, the Xbox Series X/S[b]) are home video game consoles developed by Microsoft. They were both released on November 10, 2020 as the fourth generation of the Xbox console family, succeeding the Xbox One family. Along with Sony's PlayStation 5, also released in November 2020, the Xbox Series X and Series S are part of the ninth generation of video game consoles.[3]

            Rumors regarding the consoles first emerged in early 2019, with the line as a whole codenamed "Scarlett", and consisting of high-end and lower-end models codenamed "Anaconda" and "Lockhart" respectively. Internally, Microsoft had been satisfied with the two-console approach for the Xbox One, and planned a similar approach for the fourth generation Xbox, with the target for the high-end model to at least double the performance of the Xbox One X. The high-end model was first teased during E3 2019 under the title "Project Scarlett", while its name and design as Xbox Series X was unveiled during The Game Awards later in December. In September 2020, Microsoft unveiled the lower-end model as the Xbox Series S.

            The Xbox Series X has higher end hardware, and supports higher display resolutions (up to 8K resolution) along with higher frame rates and real-time ray tracing; it also has a high-speed solid-state drive to reduce loading times. The less expensive Xbox Series S uses the same CPU, but has a less powerful GPU, has less memory and internal storage, and lacks an optical drive. Both consoles are backwards compatible with many previous generation Xbox games, controllers, and accessories. As part of a program Microsoft calls "Smart Delivery", many previous generation games feature upgraded graphics on the Series X/S at no additional charge. The consoles are also compatible with the gaming subscription service Xbox Game Pass, as well as the cloud game-streaming platform Xbox Game Pass cloud gaming."""
#corpus = """Keyword extraction is not that difficult after all. There are many libraries that 
#            can help you with keyword extraction. 
#            Rapid automatic keyword extraction is one of those."""

wikiCorpusSen = " ".join(corpus.split("\n"))
wikiCorpusSen = wikiCorpusSen.split(".")
def makeInputData(corpus, window) :
  xinputraw = np.array(corpus.lower().split(" "))
  togetherData = []
  enc = OneHotEncoder(sparse=False)
  enc.fit(xinputraw.reshape(-1,1))
  for c in range(len(xinputraw)):
    centreWord = xinputraw[c]
    contextIndices = []

    for co in range(c-window, c+window+1):
      if co == c or co <0 or co >= len(xinputraw):
        continue
      else :
        contextWord = xinputraw[co]
        togetherData.append((centreWord, contextWord))
  
  togetherData = list(set(togetherData))
  centreWordsAlone = []
  contextWordsAlone = []
  for dp in togetherData :
    centreWordsAlone.append(dp[0])
    contextWordsAlone.append(dp[1])
  
  xinput = enc.transform(np.array(centreWordsAlone).reshape(-1,1)).T
  ylabel = enc.transform(np.array(contextWordsAlone).reshape(-1,1)).T


  return xinput, ylabel

xinput, ylabel = makeInputData(corpus, 2)
outputsize = ylabel.shape[0]

xtrain, xtest, ytrain, ytest = train_test_split(xinput.T, ylabel.T, test_size=0.1)

model = Sequential([
                    Dense(30, input_shape=(outputsize,), activation="relu"),
                    Dense(10,  activation="relu"),
                    Dense(outputsize, activation="softmax")     
])
model.summary()

model.compile(optimizer=Adam(learning_rate=0.0002), loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(x=xtrain, y=ytrain, batch_size=32, epochs=1000, verbose=2)

predictions = model.predict(x=enc.transform([["real-time"], ["sony's"], ["higher"], ["resolutions"], ["\"smart"], ["game"], ["subscription"] ]), verbose=2)
#predictions1 = model.evaluate(x=xtest, y=ytest)

predictions[1:] = predictions[1:] >=0.5
print(predictions[1,:])

"""## **Semi GloVe**"""

class vec_sGV():
  def __init__(self, name):
    self.modelName = name
    self.coMatrix = None
    self.vocab = None
    self.corpus = None
    self.linedVocab = None
    self.tokenDict = None
    self.vectorDict = None
    self.w2token = None

  def removeEle(self, li, ele):
    l = li
    try:
        while True:
            l.remove(ele)
    except ValueError:
        pass
    
    return l
    
  
  
  def uniquefy(self, seq):
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not (x in seen or seen_add(x))]
  
# Removing punctuations in string
# Using loop + punctuation string
    for ele in string: 
      if ele in punc: 
        string = string.replace(ele, "")
  
    return string

  def makeCorpus(self, title=None, fromText=None):
    
    punc = '''!()-[]{};:'"\, <>./?@#$%^&*_~'''

    if fromText == None:
      titlePage = wikipedia.page(title)
      corpus = (titlePage.content).lower()
  
    else:
      corpus = fromText.lower()


    corpus = re.sub(r'==.*?==+', '', corpus)
    corpus = re.sub(r'[,;:\-\(\)\]\[\$%&\*_!<>@#"]','', corpus)
    corpus1 = re.sub(r'\n',' ', corpus)
    corpus2 = re.split(r'\.\s', corpus1)
    
    self.corpus = corpus1

    return corpus1, corpus2


  def setCoMatrix(self):

    data = []
    tokenDict = dict()
    wordIndexDict = dict()

    for wds in range(len(self.vocab)):
      tokenDict[wds] = self.vocab[wds]
    
    for wds in range(len(self.vocab)):
      wordIndexDict[self.vocab[wds]] = wds
    
    self.tokenDict = tokenDict
    self.w2token = wordIndexDict
    for lis in self.linedVocab.values():
      data += lis
    N = len(self.vocab)
    coMat = np.empty((N,N))

    for wd1 in self.vocab:
      subDict = dict()
      for wd2 in self.vocab:
        coOccur = 0
        
        for wdi in range(len(data)):
            windowWords = data[max(0,wdi-1):min(wdi+1, len(data))]
            if wd1 in windowWords and wd2 in windowWords:
              coOccur += 1
        coMat[wordIndexDict[wd1],wordIndexDict[wd2]] = coOccur
    for r in range(N):
      coMat[r,r] = 0
      
    self.coMatrix = coMat

    return coMat
        


  def makeVectors(self, corpusSen, vecSize=5, allowStopWords = False):
  
   
    noUseWords = stopwords.words('english')
    togetherData = []

    vocab = []
    vocabLineWise = dict()
    lineCount = 0
    for lines in corpusSen :
      lines = re.sub("((\s+)\s)"," ", lines)
      lines = re.sub("^(\s+)|(\s+)$|","", lines)
      words = lines.split(" ")
      vocab += words
      vocabLineWise[lineCount] = words
      lineCount += 1
    
    vocab = self.uniquefy(vocab)
    if allowStopWords == False:
      for nouse in noUseWords:
        if nouse in vocab:
          vocab = self.removeEle(vocab, nouse)
    
    for l in range(lineCount):
      temp = copy.deepcopy(vocabLineWise[l])
      for wds in vocabLineWise[l]:
      
        if wds not in vocab:
          
          temp = self.removeEle(temp, wds)
        
      vocabLineWise[l] = temp
      
    self.vocab = vocab
    self.linedVocab = vocabLineWise
    coMatrix = self.setCoMatrix()

    N = coMatrix.shape[1]
    vecToken = 0
    vecDict = dict()
    for r in range(N):
      vecDict[self.tokenDict[vecToken]] = coMatrix[r,:]
      vecToken += 1
    
    if vecSize <N:
      labels =[]
      tokens= []

      for word in self.vocab:
        tokens.append(list(vecDict[word]))
        labels.append(word)

    
    #print(tokens[0:2])
      tokens = np.array(tokens).reshape(-1,len(tokens))
      Sved,S,_ = np.linalg.svd(tokens, full_matrices=False)
      #Vtsned = TSNE(n_components=vecSize, method="exact").fit_transform(tokens)

      vecToken = 0
      for r in range(N):
        vecDict[self.tokenDict[vecToken]] = np.dot(tokens[r], Sved[:,:vecSize])
        vecToken += 1
        
        

    self.vectorDict = vecDict
    return vecDict

  def mostSimilar(self, word1, topN=1):

    scores = []
    word1 = self.vectorDict[word1]/np.linalg.norm(self.vectorDict[word1])
    for i in range(len(self.vectorDict)):
      word2 = self.vectorDict[self.tokenDict[i]]/np.linalg.norm(self.vectorDict[self.tokenDict[i]])
      DOT = np.dot(word1,word2)
      scores.append(DOT)
    scores = np.array(scores)
    scores1 = np.flip(np.argsort(scores))[:topN]
    #scores2 = np.flip(np.sort(scores))[:topN]

    for i in scores1:
      print( (self.tokenDict[i], scores[i]))
    
    return


  def visualiseVectors(self, targetDimensions=2, vecCount =None):
    labels =[]
    tokens= []

    for word in self.vocab:
        tokens.append(self.vectorDict[word])
        labels.append(word)

    
    #print(tokens[0:2])
    Vtsned = TSNE(n_components=targetDimensions, method="exact").fit_transform(tokens)

    x = []
    y = []
    for value in Vtsned:
        x.append(value[0])
        y.append(value[1])
        
    plt.figure(figsize=(16, 16))
    if vecCount == None:
      num = len(x)
    else:
      num = vecCount 
    for i in range(num):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

    return

"""### **My Predictions**"""

corpus = """Viruses are microscopic organisms that exist almost everywhere on earth. They can infect animals, plants, fungi, and even bacteria.

Sometimes a virus can cause a disease so deadly that it is fatal. Other viral infections trigger no noticeable reaction.

A virus may also have one effect on one type of organism, but a different effect on another. This explains how a virus that affects a cat may not affect a dog.

Viruses vary in complexity. They consist of genetic material, RNA or DNA, surrounded by a coat of protein, lipid (fat), or glycoprotein. Viruses cannot replicate without a host, so they are classified as parasitic.

They are considered the most abundant biological entity on the planet.Almost every ecosystem on Earth contains viruses.

Before entering a cell, viruses existTrusted Source in a form known as virions.

During this phase, they are roughly one-hundredth the size of a bacterium and consist of two or three distinct parts:

genetic material, either DNA or RNA
a protein coat, or capsid, which protects the genetic information
a lipid envelope is sometimes present around the protein coat when the virus is outside of the cell
Viruses do not contain a ribosome, so they cannot make proteins. This makes them totally dependent on their host. They are the only type of microorganism that cannot reproduce without a host cell.

After contacting a host cell, a virus will insert genetic material into the host and take over that hostâ€™s functions.

After infecting the cell, the virus continues to reproduce, but it produces more viral protein and genetic material instead of the usual cellular products.

It is this process that earns viruses the classification of parasite.

Viruses have different shapes and sizes, and they can be categorized by their shapes.

These may be:

Helical: The tobacco mosaic virus has a helix shape.
Icosahedral, near-spherical viruses: Most animal viruses are like this.
Envelope: Some viruses cover themselves with a modified section of cell membrane, creating a protective lipid envelope. These include the influenza virus and HIV.A virus exists only to reproduce. When it reproduces, its offspring spread to new cells and new hosts.

The makeup of a virus affects its ability to spread.

Viruses may transmit from person to person, and from mother to child during pregnancy or delivery.

They can spread through:

touch
exchanges of saliva, coughing, or sneezing
sexual contact
contaminated food or water
insects that carry them from one person to another
Some viruses can live on an object for some time, so if a person touches an item with the virus on their hands, the next person can pick up that virus by touching the same object. The object is known as a fomite.

As the virus replicates in the body, it starts to affect the host. After a period known as the incubation period, symptoms may start to show.

What happens if viruses change?
When a virus spreads, it can pick up some of its hostâ€™s DNA and take it to another cell or organism.

If the virus enters the hostâ€™s DNA, it can affect the wider genome by moving around a chromosome or to a new chromosome.

This can have long-term effects on a person. In humans, it may explain the development of hemophilia and muscular dystrophy.

This interaction with host DNA can also cause viruses to change.

Some viruses only affect one type of being, say, birds. If a virus that normally affects birds does by chance enter a human, and if it picks up some human DNA, this can produce a new type of virus that may be more likely to affect humans in future.

This is why scientists are concerned about rare viruses that spread from animals to people."""
#corpus = ".".join(corpus.split("\n"))

sglove = vec_sGV("MYsemiglove")
_, corpusSen = sglove.makeCorpus(fromText=corpus)
_ = sglove.makeVectors(corpusSen=corpusSen, vecSize=10)
#_ = sglove.visualiseVectors(targetDimensions=2)
_ = sglove.mostSimilar(word1="helix", topN=5)

_ = sglove.mostSimilar(word1="dna", topN=10)

word1 = sglove.coMatrix[:, sglove.w2token["dna"]]/np.linalg.norm(sglove.coMatrix[:, sglove.w2token["dna"]])

word2 = sglove.coMatrix[:, sglove.w2token["picks"]]/np.linalg.norm(sglove.coMatrix[:, sglove.w2token["picks"]])

"""# **Chat Predict With RNNS**"""

chat = open('/content/jaymama_chat.txt')
data = chat.read()
data = data.splitlines()
chat.close()

clean_data = []
pattern = "\[([^\[\]]+|(?0))*]"
for chat in data :
  rechat = re.sub("[\(\[].*?[\)\]]", "", chat) 
  if "\u200e" in rechat or "Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them." in rechat or len(rechat.split(" "))<=1 or "https" in rechat:
    continue
  clean_data.append(rechat)

corpus_w2v = " ".join(clean_data)

chat_predict_w2v = vecW2V("chat_prediction")

_, corpus_sen_w2v = chat_predict_w2v.makeCorpus(fromText=corpus_w2v)

model_w2v, input_dict_w2v = chat_predict_w2v.makeModelAndInput(80, corpus_sen_w2v, window=5)

model_w2v = chat_predict_w2v.train()

"""
lined_vocab_w2v = chat_predict_w2v.modelLinedVocab

corpus_rnn = []
for lines in lined_vocab_w2v.values():
  corpus_rnn.append(chat_predict_w2v(lines))

print(corpus_rnn)"""

var = "end"

chat_predict_w2v.train(epochs=1)

from statistics import mean

def normalize_vecs(unnorm_vec):
  norm_vec = unnorm_vec/np.linalg.norm(unnorm_vec,keepdims=True)
  return norm_vec

def set_inputs_right(w2v_obj, seq_len="auto"):
  
  global X_in_word
  global Y_in_word
  
  lined_vocab_w2v = w2v_obj.modelLinedVocab

  corpus_rnn = []
  corpus_rnn_txt = []
  for lines in lined_vocab_w2v.values():
    result_txt = [line for line in lines]
    result = [normalize_vecs(line).reshape(-1).tolist() for line in w2v_obj(lines)]
    corpus_rnn.append(result)
    corpus_rnn_txt.append(result_txt)
  
  
  
  sen_lens = []
  for sen in corpus_rnn:
    sen_lens.append(len(sen))
  if seq_len == "auto":
    seq_len = int(mean(sen_lens)/3)
  

  X = []
  X_in_word = []
  Y = []
  Y_in_word = []

  for sen in corpus_rnn:
    #seq_len = 50
    num_records = len(sen) - seq_len
    #senc = sen.tolist()
    for i in range(num_records):
        X.append(sen[i:i+seq_len])
        Y.append(sen[i+seq_len])

  for sen in corpus_rnn_txt:
    #seq_len = 50
    num_records = len(sen) - seq_len
    #senc = sen.tolist()
    for i in range(num_records):
        X_in_word.append(sen[i:i+seq_len])
        Y_in_word.append(sen[i+seq_len])
        
  
  X = np.array(X)
  X_in_word = np.array(X_in_word)
  #X = np.expand_dims(X, axis=2)
  

  Y = np.array(Y)
  Y_in_word = np.array(Y_in_word)
  #Y = np.expand_dims(Y, axis=1)
  

  return X, Y, corpus_rnn, corpus_rnn_txt
  
def performance_by_similarity(true_vector, predicted_vector):
  score = np.dot(true_vector, predicted_vector.T)
  den = np.linalg.norm(true_vector)*np.linalg.norm(predicted_vector)
  score = score/den

  return score
def predict_next_words(w2v_obj, prediction_vector, sequence_index=-1, topN=10, printList=True):
  
  matches = [] #np.dot(w2v_obj.modelWordVectors, prediction_vector)
  for vec in range(w2v_obj.modelWordVectors.shape[0]):
    vector = w2v_obj.modelWordVectors[vec]
    matches.append(performance_by_similarity(vector, prediction_vector))
  
  reqMatches = np.flip(np.argsort(matches))[0:topN]

  reqProbs = np.flip(np.sort(matches))[0:topN]

  originWord2 = np.zeros((w2v_obj.modelInputDict["Input1"].shape[1],1))
  
  count = 0
  result = []
  result_in_vector = []
  if sequence_index  == -1:
    if printList == True:
      print("Out Of Input Prequel Words has following Predictions : \n")
    for vindex in reqMatches :
      originWord2[vindex,0] = 1
    
      result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
      result_in_vector.append(w2v_obj.modelWordVectors[vindex])
      if printList == True:
        print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
      count += 1
      originWord2[vindex,0] = 0
  else:
    if printList == True:
      for wds in X_in_word[sequence_index]:#corpus_rnn_txt[sequence_index//divide_factor][sequence_index*seq_len :seq_len*(1+sequence_index)]:
        print(wds, end="-")
      print("\u0332".join(Y_in_word[sequence_index]+" "))

    for vindex in reqMatches :
        originWord2[vindex,0] = 1
      
        result.append((enc.inverse_transform(np.array([originWord2]).reshape(1,-1))[0,:], reqProbs[count]))
        result_in_vector.append(w2v_obj.modelWordVectors[vindex])
        if printList == True:
          print(enc.inverse_transform(np.array([originWord2]).reshape(1,-1)), reqProbs[count])
        count += 1
        originWord2[vindex,0] = 0


  return result, result_in_vector


def auto_code_c(seq_len, w2v_obj, rnn_model):
  print("Type in comments for a code snippet with minimum", seq_len," words length :\n")
  typed = input("")
  word_list = typed.split(" ")
  auto_predict_upto = int(input("Enter upto how many words the code should be generated : "))
  for wds in typed.split(" "):
    if wds not in w2v_obj.modelVocab:
      while (wds in word_list):
        word_list.remove(wds)
  typed = " ".join(word_list)

  if len(word_list) <seq_len:
    print("Insufficent vocab words in typed content")

  else:
    typed_txt = typed.split(" ")
    typed = [normalize_vecs(wd) for wd in w2v_obj(typed.split(" "))]

    X_custom = []
    X_custom_in_word = []
    num_records = len(typed) - seq_len +1
      
    for i in range(num_records):
        X_custom.append(typed[i:i+seq_len])
        X_custom_in_word.append(typed_txt[i:i+seq_len])

    m  = rnn_model.input_shape[-1]
    X_custom = np.array(X_custom).reshape(-1, seq_len, m)
    
    X_custom_cache = [] + list(X_custom.reshape(-1, m)[:seq_len])
  
    pred_cache = []
    print("Generated code snippet : \n")
    if auto_predict_upto == -1:
      pred= "start"
      i = 0
      while (pred != "#end" and pred != "#python"):
        consider = np.array(X_custom_cache[len(X_custom_cache)-seq_len:])
    
        pred, _ = predict_next_words(w2v_obj, rnn_model.predict(np.expand_dims(consider, axis=0))[0], topN=1, printList=False)
        pred_vector = w2v_obj(pred[0][0].tolist())[0][0]
        pred_vector = normalize_vecs(pred_vector)
        pred = pred[0][0][0]

        X_custom_cache.append(pred_vector)
        pred_cache.append(pred)
        #pred_cache.pop(0)
        X_custom_cache.pop(0)
        if (i%10 == 0):
          print("\n")
        print("", end=" ")
        print(pred, end=" ")
        i+=1

    else:
      for i in range(auto_predict_upto):
        consider = np.array(X_custom_cache[len(X_custom_cache)-seq_len:])
      
        pred, _ = predict_next_words(w2v_obj, rnn_model.predict(np.expand_dims(consider, axis=0))[0], topN=1, printList=False)
        pred_vector = w2v_obj(pred[0][0].tolist())[0][0]
        pred_vector = normalize_vecs(pred_vector)
        pred = pred[0][0][0]

        X_custom_cache.append(pred_vector)
        pred_cache.append(pred)
        #pred_cache.pop(0)
        X_custom_cache.pop(0)
        if (i%10 == 0):
          print("\n")
        print("", end=" ")
        print(pred, end=" ")

X, Y, corpus_rnn, corpus_rnn_txt = set_inputs_right(chat_predict_w2v, seq_len=4)
X.shape, Y.shape

import recurrent_neural_networks
from recurrent_neural_networks import rnns
models = rnns()
model = models.make_many_one_model(serial_input_card=4,architecture_per_ss=[('inpk',80),('jnk', 160),('ffk', 40),('ffk', 80)], activation="linear", hidden_activations="relu")

model.compile(optimizer=Adam(learning_rate=0.002), loss="mse", metrics=['mae'])
model.fit(X, Y, epochs=200)

scores = []
pred_vs = model.predict(np.expand_dims(X, axis=0).reshape(-1, 4, 80)) 
threshold = 0.7
for sam in range(X.shape[0]):
  pred_v = pred_vs[sam]
  score = performance_by_similarity(Y[sam], pred_v)
  scores.append(score)

best_preds_seq_nums = []
count =0
for sc in scores:
  if sc >=threshold:
    best_preds_seq_nums.append(count)
  count +=1

print(best_preds_seq_nums)
print(len(best_preds_seq_nums)/len(scores)*100, "% samples produced predicted words with",threshold*100, "% match with the original word")
print("NOTE : 60% match between original and predicted word would be enough to rightly predict the word")

text =" ".join(Y_in_word[0:500])
#text = Y_in_word[0:500]
print("Original Text upto ", 500,"words :\n", text)

gen_text = []
for wds in range(0, 500):
  next_wds = predict_next_words(chat_predict_w2v, wds,model.predict(np.expand_dims(X[wds], axis=0))[0], printList=False, topN=1)[0][0]
  
  gen_text += list(next_wds)

gen_text = " ".join(gen_text)

print("AI Generated Text upto", 500,"words :\n", gen_text)
"""
print("Matched Original And Generated Words\n")
count = 1
for wds in range(0,500):
  if text[wds] == gen_text[wds]:
    print(count, ":", text[wds])
    count +=1
"""
a = "end"

index = 106
_ = predict_next_words(chat_predict_w2v, model.predict(np.expand_dims(X[index], axis=0))[0], index)

typed = input("Type in content with minimum 4 words length :\n")
seq_len = 4
word_list = typed.split(" ")
for wds in typed.split(" "):
  if wds not in chat_predict_w2v.modelVocab:
    while (wds in word_list):
      word_list.remove(wds)
typed = " ".join(word_list)

if len(typed) <4:
  raise "Insufficent vocab words in typed content"

typed_txt = typed.split(" ")
typed = [normalize_vecs(wd) for wd in chat_predict_w2v(typed.split(" "))]

X_custom = []
X_custom_in_word = []
num_records = len(typed) - seq_len +1
  
for i in range(num_records):
    X_custom.append(typed[i:i+seq_len])
    X_custom_in_word.append(typed_txt[i:i+seq_len])


X_custom = np.array(X_custom).reshape(-1, 4, 80)


print("predictions : \n")
for index in range(X_custom.shape[0]):
  pred = predict_next_words(chat_predict_w2v, model.predict(np.expand_dims(X_custom[index], axis=0))[0], topN=3, printList=False)
  
  print(" ".join(X_custom_in_word[index]), ":", pred)



"""# **Code Predict with RNNs**"""

code = open('/content/python codes.txt')
data = code.read()
data = data.splitlines()
code.close()

clean_data = []
pattern = "" #"\[([^\[\]]+|(?0))*]"
for chat in data :
  rechat = chat #re.sub("[\(\[].*?[\)\]]", "", chat) 
 
  clean_data.append(rechat)

clean_data

corpus_w2v = " ".join(clean_data)

code_predict_w2v = vecW2V("code_prediction")

_, corpus_sen_w2v = code_predict_w2v.makeCorpus(fromText=corpus_w2v, allowPunctuation=True, allowLineBreaks=False)

model_w2v, input_dict_w2v = code_predict_w2v.makeModelAndInput(50, corpus_sen_w2v, window=6, negativeSampling=6, allowStopWords=True)

model_w2v = code_predict_w2v.train()

X, Y, corpus_rnn, corpus_rnn_txt = set_inputs_right(code_predict_w2v, seq_len=6)
X.shape, Y.shape

import recurrent_neural_networks
from recurrent_neural_networks import rnns
models = rnns()
model = models.make_many_one_model(serial_input_card=6,architecture_per_ss=[('inpk',50),('jnk', 100),('ffk', 50)], activation="linear", hidden_activations="relu")

model.compile(optimizer=Adam(learning_rate=0.0001), loss="mse", metrics=['mae'])
model.fit(X, Y, epochs=500)

scores = []
pred_vs = model.predict(np.expand_dims(X, axis=0).reshape(-1, 6, 50)) 
threshold = 0.9
for sam in range(X.shape[0]):
  pred_v = pred_vs[sam]
  score = performance_by_similarity(Y[sam], pred_v)
  scores.append(score)

best_preds_seq_nums = []
count =0
for sc in scores:
  if sc >=threshold:
    best_preds_seq_nums.append(count)
  count +=1

print(best_preds_seq_nums)
print(len(best_preds_seq_nums)/len(scores)*100, "% samples produced predicted words with",threshold*100, "% match with the original word")
print("NOTE : 60% match between original and predicted word would be enough to rightly predict the word")

text =" ".join(Y_in_word[0:500])
#text = Y_in_word[0:500]
print("Original Text upto ", 500,"words :\n", text)

gen_text = []
for wds in range(0, 500):
  next_wds,_ = predict_next_words(code_predict_w2v,model.predict(np.expand_dims(X[wds], axis=0))[0], printList=False, topN=1, sequence_index=wds)[0][0]
  
  gen_text += list(next_wds)

gen_text = " ".join(gen_text)

print("AI Generated Text upto", 500,"words :\n", gen_text)
"""
print("Matched Original And Generated Words\n")
count = 1
for wds in range(0,500):
  if text[wds] == gen_text[wds]:
    print(count, ":", text[wds])
    count +=1
"""
a = "end"

index = 190
_ = predict_next_words(code_predict_w2v, model.predict(np.expand_dims(X[index], axis=0))[0], index)

auto_code_c(6, code_predict_w2v, model)

clean_data

"/* User defined function sum that has two int * parameters. The function adds these numbers and * return the result of addition. */"
typed = input("Type in content with minimum 5 words length :\n")
seq_len = 5
word_list = typed.split(" ")
for wds in typed.split(" "):
  if wds not in code_predict_w2v.modelVocab:
    while (wds in word_list):
      word_list.remove(wds)
typed = " ".join(word_list)

if len(word_list) <seq_len:
  print("Insufficent vocab words in typed content")

else:
  typed_txt = typed.split(" ")
  typed = [normalize_vecs(wd) for wd in code_predict_w2v(typed.split(" "))]

  X_custom = []
  X_custom_in_word = []
  num_records = len(typed) - seq_len +1
    
  for i in range(num_records):
      X_custom.append(typed[i:i+seq_len])
      X_custom_in_word.append(typed_txt[i:i+seq_len])


  X_custom = np.array(X_custom).reshape(-1, 5, 30)

 
  print("predictions : \n")
  for index in range(X_custom.shape[0]):
    pred,_ = predict_next_words(code_predict_w2v, model.predict(np.expand_dims(X_custom[index], axis=0))[0], topN=1, printList=False)
    print(" ".join(X_custom_in_word[index]), ":", pred)



"""# **Sentiment Analysis With RNNs**"""

!pip install wikipedia

#from myword2vec import *
#from recurrent_neural_networks import *
import nltk
from nltk.corpus import movie_reviews
nltk.download('movie_reviews')

def word_feats(words):
    return dict([(word, True) for word in words])
 
negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')
 
negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]
 
negcutoff = int(len(negfeats)*3/4)
poscutoff = int(len(posfeats)*3/4)
 
trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
print("train on %d instances, test on %d instances" % (len(trainfeats), len(testfeats)))

reviews_cons = 10

entire_corpus = " ".join(movie_reviews.words(fileids=movie_reviews.fileids('pos')[:reviews_cons]))
entire_corpus += " ".join(movie_reviews.words(fileids=movie_reviews.fileids('neg')[:reviews_cons]))
words_in_entire_corpus = entire_corpus.split(" ")
print(len(words_in_entire_corpus))
print(len(set(words_in_entire_corpus)))

# run this from a normal command line
!python -m spacy download en_core_web_md

import spacy

# Load the spacy model that you have installed
nlp = spacy.load('en_core_web_md')

# process a sentence using the model
doc = nlp("red ball is played in test. white ball is played in odi. blue ball is played in pool games.")

# It's that simple - all of the vectors and words are assigned after this point
# Get the vector for 'text':
doc[3].vector

# Get the mean vector for the entire sentence (useful for sentence classification etc.)
doc.vector

reviewer = vecW2V("movie_reviewer")

_, corpus_in_sen = reviewer.makeCorpus(fromText=entire_corpus)
model_rev, input_dict = reviewer.makeModelAndInput(vecSize=30, window=6, corpus2=corpus_in_sen)
model_rev = reviewer.train(epochs=10)

model_rev = reviewer.train(epochs=50)

reviewer.modelLinedVocab

reviewer.autoFillList(word1="american", topN=20)

import numpy as np

sam = 2*10
seq_len = 5 #splitting every review as ten portions
sequences = []
sen_global_c = 0

Y = []
rev_vecs = []
for f in range(sam):
   rev = " ".join(movie_reviews.words(fileids=movie_reviews.fileids((f>=0 and f<sam//2)*'pos' + (f>=sam//2 and f<sam)*'neg')[f%(sam//2)]))
   rev_words = rev.split(" ")
   
   rev_words = [wd for wd in rev_words if wd in reviewer.modelVocab]
   per_seq = len(rev_words)//seq_len
   rev_vecs_i = reviewer(rev_words)

   vecs_in_seq = []
   for seq in range(seq_len):

     seq_vec = 0
     seq_vecs = rev_vecs_i[seq*per_seq : (seq+1)*per_seq]
     
     for sv in seq_vecs:
       seq_vec += sv
     seq_vec = seq_vec/per_seq
     vecs_in_seq.append(seq_vec.tolist())
   Y.append((f>=0 and f<sam//2)*1 + (f>=sam//2 and f<sam)*0)
   rev_vecs.append(vecs_in_seq)

   





X = np.array(rev_vecs)

X = np.squeeze(X, axis=2)
Y= np.array(Y)
Y = np.expand_dims(Y, axis=1)
X.shape, Y.shape

rnn = rnns()
model_rnn = rnn.make_many_one_model(serial_input_card=5, architecture_per_ss=[('inpk', 30),('drp', 0.2),('jnk', 60), ('drp', 0.2), ('ffk', 1)])

xtrain, xtest, ytrain, ytest = train_test_split(X, Y)

xtrain.shape, xtest.shape, ytrain.shape, ytest.shape

model_rnn.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs=100, verbose=1)

ytrain[:10], model_rnn.predict(xtrain[:10])

